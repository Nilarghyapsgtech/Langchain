{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92cd08cf",
   "metadata": {},
   "source": [
    "MMR retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "embeddings=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "vector_db=Chroma(\n",
    "    collection_name='chromaCollection',\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory='vector_store'\n",
    ")\n",
    "\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"RAG reduces the cost of fine-tuning by grounding LLMs with external data.\",\n",
    "        metadata={\"topic\": \"RAG\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain uses vector embeddings to enable semantic search over documents.\",\n",
    "        metadata={\"topic\": \"LangChain\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector embeddings convert text into high-dimensional numerical representations.\",\n",
    "        metadata={\"topic\": \"Embeddings\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"FAISS is commonly used as a vector store for fast similarity search in RAG systems.\",\n",
    "        metadata={\"topic\": \"VectorDB\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Chunking improves retrieval accuracy by splitting large documents into smaller pieces.\",\n",
    "        metadata={\"topic\": \"Text Chunking\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cosine similarity is widely used to measure closeness between embedding vectors.\",\n",
    "        metadata={\"topic\": \"Similarity Search\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG pipelines consist of retrievers, prompt templates, and language models.\",\n",
    "        metadata={\"topic\": \"RAG Architecture\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Dense Passage Retrieval enables semantic search using bi-encoder architectures.\",\n",
    "        metadata={\"topic\": \"Retrieval\", \"difficulty\": \"Advanced\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Prompt engineering helps control LLM outputs without retraining the model.\",\n",
    "        metadata={\"topic\": \"Prompt Engineering\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain supports chaining, memory, tools, and agents for complex workflows.\",\n",
    "        metadata={\"topic\": \"LangChain\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Hybrid search combines keyword-based and vector-based retrieval techniques.\",\n",
    "        metadata={\"topic\": \"Search\", \"difficulty\": \"Advanced\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Metadata filtering allows retrieval of documents based on structured attributes.\",\n",
    "        metadata={\"topic\": \"Metadata\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Reranking improves answer quality by reordering retrieved documents using cross-encoders.\",\n",
    "        metadata={\"topic\": \"RAG Optimization\", \"difficulty\": \"Advanced\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "vector_db.add_documents(sample_docs)\n",
    "\n",
    "mmr_retriever=vector_db.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "    'k':3,                                              #Final Nos of Results\n",
    "    'fetch_k':5,                                        # Initial Pool to select from\n",
    "    'lambda_mult':0.6                                   # 0-diverse and 1-Relevnce\n",
    "    }\n",
    ")\n",
    "\n",
    "query=\"How can we reduce the Cost of Fine Tuning\"\n",
    "\n",
    "result=mmr_retriever.invoke(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af649d",
   "metadata": {},
   "source": [
    "Custom Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407058ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG reduces the cost of fine-tuning by grounding LLMs with external data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "vector_db=Chroma(\n",
    "    collection_name='vectordb',\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory='vector_store'\n",
    ")\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"RAG reduces the cost of fine-tuning by grounding LLMs with external data.\",\n",
    "        metadata={\"topic\": \"RAG\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain uses vector embeddings to enable semantic search over documents.\",\n",
    "        metadata={\"topic\": \"LangChain\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector embeddings convert text into high-dimensional numerical representations.\",\n",
    "        metadata={\"topic\": \"Embeddings\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"FAISS is commonly used as a vector store for fast similarity search in RAG systems.\",\n",
    "        metadata={\"topic\": \"VectorDB\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Chunking improves retrieval accuracy by splitting large documents into smaller pieces.\",\n",
    "        metadata={\"topic\": \"Text Chunking\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cosine similarity is widely used to measure closeness between embedding vectors.\",\n",
    "        metadata={\"topic\": \"Similarity Search\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG pipelines consist of retrievers, prompt templates, and language models.\",\n",
    "        metadata={\"topic\": \"RAG Architecture\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Dense Passage Retrieval enables semantic search using bi-encoder architectures.\",\n",
    "        metadata={\"topic\": \"Retrieval\", \"difficulty\": \"Advanced\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Prompt engineering helps control LLM outputs without retraining the model.\",\n",
    "        metadata={\"topic\": \"Prompt Engineering\", \"difficulty\": \"Easy\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain supports chaining, memory, tools, and agents for complex workflows.\",\n",
    "        metadata={\"topic\": \"LangChain\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Hybrid search combines keyword-based and vector-based retrieval techniques.\",\n",
    "        metadata={\"topic\": \"Search\", \"difficulty\": \"Advanced\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Metadata filtering allows retrieval of documents based on structured attributes.\",\n",
    "        metadata={\"topic\": \"Metadata\", \"difficulty\": \"Intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Reranking improves answer quality by reordering retrieved documents using cross-encoders.\",\n",
    "        metadata={\"topic\": \"RAG Optimization\", \"difficulty\": \"Advanced\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "vector_db.add_documents(sample_docs)\n",
    "\n",
    "@chain\n",
    "def custom_retriever(query:str):\n",
    "    \"\"\"\n",
    "    Custom retriever rules:\n",
    "    1)Get relevant Docs from the Vector Store\n",
    "    2)Get them Filtered By metadata\n",
    "    3)Return Top Results\n",
    "    \"\"\"\n",
    "\n",
    "    results=vector_db.similarity_search(query,k=5)\n",
    "    filtered_docs=[doc for doc in results if doc.metadata.get('topic')=='RAG']\n",
    "    return filtered_docs[:2]\n",
    "\n",
    "query=\"How can we reduce the Cost of Fine Tuning\"\n",
    "\n",
    "result=custom_retriever.invoke(query)\n",
    "print(result[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75226c",
   "metadata": {},
   "source": [
    "Arxiv Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2fab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Entry ID': 'http://arxiv.org/abs/2306.05212v1', 'Published': datetime.date(2023, 6, 8), 'Title': 'RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit', 'Authors': 'Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, Ji-Rong Wen'}\n",
      "{'Entry ID': 'http://arxiv.org/abs/2407.07093v1', 'Published': datetime.date(2024, 7, 9), 'Title': 'FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation', 'Authors': 'Liqun Ma, Mingjie Sun, Zhiqiang Shen'}\n",
      "{'Entry ID': 'http://arxiv.org/abs/2408.13006v2', 'Published': datetime.date(2025, 3, 30), 'Title': 'Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates', 'Authors': 'Hui Wei, Shenghua He, Tian Xia, Fei Liu, Andy Wong, Jingyang Lin, Mei Han'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "arxiv_retriever=ArxivRetriever(load_max_docs=5,load_all_available_meta=True)\n",
    "\n",
    "query=\"LLM\"\n",
    "\n",
    "result=arxiv_retriever.invoke(query)\n",
    "for src in result:\n",
    "    print(src.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98908cb6",
   "metadata": {},
   "source": [
    "Batch arxiv query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='This work presents a Fully BInarized Large Language Model (FBI-LLM), demonstrating for the first time how to train a large-scale binary language model from scratch (not the partial binary or ternary LLM like BitNet b1.58) to match the performance of its full-precision counterparts (e.g., FP16 or BF16) in transformer-based LLMs. It achieves this by employing an autoregressive distillation (AD) loss with maintaining equivalent model dimensions (130M, 1.3B, 7B) and training data volume as regular LLM pretraining, while delivering competitive results in terms of perplexity and task-specific effectiveness. Intriguingly, by analyzing the training trajectory, we find that the pretrained weight is not necessary for training binarized LLMs from scratch. This research encourages a new computational framework and may facilitate the future design of specialized hardware tailored for fully 1-bit LLMs. We make all models, code, and training dataset fully accessible and transparent to support further research (Code: https://github.com/LiqunMa/FBI-LLM. Model: https://huggingface.co/LiqunMa/).' metadata={'Entry ID': 'http://arxiv.org/abs/2407.07093v1', 'Published': datetime.date(2024, 7, 9), 'Title': 'FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation', 'Authors': 'Liqun Ma, Mingjie Sun, Zhiqiang Shen'}\n",
      "page_content='This paper introduces the R package slm which stands for Stationary Linear Models. The package contains a set of statistical procedures for linear regression in the general context where the error process is strictly stationary with short memory. We work in the setting of Hannan (1973), who proved the asymptotic normality of the (normalized) least squares estimators (LSE) under very mild conditions on the error process. We propose different ways to estimate the asymptotic covariance matrix of the LSE, and then to correct the type I error rates of the usual tests on the parameters (as well as confidence intervals). The procedures are evaluated through different sets of simulations, and two examples of real datasets are studied.' metadata={'Entry ID': 'http://arxiv.org/abs/1906.06583v3', 'Published': datetime.date(2019, 10, 22), 'Title': 'Linear regression with stationary errors : the R package slm', 'Authors': 'Emmanuel Caron, Jérôme Dedecker, Bertrand Michel'}\n",
      "page_content='In this paper, we study the role of initialization in Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from the pretrained model as initialization for finetuning, one can either initialize B to zero and A to random (default initialization in PEFT package), or vice-versa. In both cases, the product BA is equal to zero at initialization, which makes finetuning starts from the pretrained model. These two initialization schemes are seemingly similar. They should in-principle yield the same performance and share the same optimal learning rate. We demonstrate that this is an incorrect intuition and that the first scheme (initializing B to zero and A to random) on average yields better performance compared to the other scheme. Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme. We validate our results with extensive experiments on LLMs.' metadata={'Entry ID': 'http://arxiv.org/abs/2406.08447v1', 'Published': datetime.date(2024, 6, 12), 'Title': 'The Impact of Initialization on LoRA Finetuning Dynamics', 'Authors': 'Soufiane Hayou, Nikhil Ghosh, Bin Yu'}\n",
      "page_content='Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.' metadata={'Entry ID': 'http://arxiv.org/abs/2401.15391v1', 'Published': datetime.date(2024, 1, 27), 'Title': 'MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries', 'Authors': 'Yixuan Tang, Yi Yang'}\n"
     ]
    }
   ],
   "source": [
    "queries=[\n",
    "\"LLM\",\n",
    "\"SLM\",\n",
    "\"Finetuning\",\n",
    "\"RAG\"\n",
    "]\n",
    "\n",
    "arxiv_retriever=ArxivRetriever(load_max_docs=3,load_all_available_meta=True)\n",
    "result=arxiv_retriever.batch(queries)                                                   # for each of the queries 3 docs are retrieved\n",
    "\n",
    "for index in range(0,len(result)):\n",
    "    print(result[index][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b45c0d",
   "metadata": {},
   "source": [
    "Tavily Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4bed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'What is a large language model (LLM)?', 'source': 'https://ask.library.arizona.edu/faq/407985', 'score': 0.99957615, 'images': []}, page_content='# What is a large language model (LLM)? A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. This refers to models that are trained on vast amounts of data and can be adapted to a wide range of tasks and operations, , not just working with language. * Beyond ChatGPT: other useful language models. ## What is a large language model (LLM)? A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. This refers to models that are trained on vast amounts of data and can be adapted to a wide range of tasks and operations, , not just working with language. * Beyond ChatGPT: other useful language models.'), Document(metadata={'title': 'What is an LLM (large language model)?', 'source': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'score': 0.99955577, 'images': []}, page_content='Large language models (LLMs) are machine learning models that can comprehend and generate human language text. A large language model (LLM) is a type of artificial intelligence (AI) program that can recognize and generate text, among other tasks. LLMs are trained on huge sets of data — hence the name \"large.\" LLMs are built on machine learning: specifically, a type of neural network called a transformer model. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set, at least at first. Any large, complex data set can be used to train LLMs, including programming languages. But LLMs use the inputs they receive to further train their models, and they are not designed to be secure vaults; they may expose confidential data in response to queries from other users. A large language model is an AI model that is trained on huge sets of data to recognize, interpret, and generate text.'), Document(metadata={'title': 'Large language model', 'source': 'https://en.wikipedia.org/wiki/Large_language_model', 'score': 0.99943846, 'images': []}, page_content='A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "retriever=TavilySearchAPIRetriever(k=3)\n",
    "query=\"What is LLM\"\n",
    "\n",
    "result=retriever.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24ae10",
   "metadata": {},
   "source": [
    "QA chain using retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8867e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Volkswagen ID. Life is a VW concept car intended to foresee an upcoming supermini car for the ID. series, potentially even replacing the Volkswagen Polo in early plans.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo',temperature=0.5)\n",
    "retriever=WikipediaRetriever(top_k_results=3,doc_content_chars_max=1500)\n",
    "\n",
    "prompt_template=\"\"\"\n",
    "Answers the Questions Based on the context from Wipepedia.\n",
    "comntext:{context}\n",
    "Question:{Question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt=PromptTemplate.from_template(prompt_template)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "docs=RunnableLambda(format_docs)\n",
    "\n",
    "chain=({\"context\":retriever|docs,\"Question\":RunnablePassthrough()}|prompt|llm|StrOutputParser())\n",
    "\n",
    "result=chain.invoke(\"What id Life\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f0d5d",
   "metadata": {},
   "source": [
    "QA chain using multiple retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ec2160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information from both Wikipedia and Arxiv, a cow can be interpreted in several ways:\n",
      "\n",
      "1. In the context of the cartoon \"Cow tools\" and the animated sitcom \"Cow and Chicken\", a cow is a farm animal typically associated with milk production and having anthropomorphic characteristics in fictional settings.\n",
      "\n",
      "2. In the HuCow BDSM subculture, a cow is a role-played character where individuals act as dairy cows, emphasizing themes of submission and objectification.\n",
      "\n",
      "3. From a broader perspective discussed in the Arxiv article, the concept of a cow can be seen as a symbol representing the evolving understanding of fundamental concepts in science and philosophy, where interpretations of what is considered \"fundamental\" may change over time based on theoretical and experimental advances.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever,ArxivRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "prompt_template=\"\"\"\n",
    "Answer the Question Based on both Context from Wikepedia and Arxiv\n",
    "Context:{Context}\n",
    "\n",
    "Question:{Question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "wiki_retriever=WikipediaRetriever(top_k_results=3)\n",
    "arxiv_retriever=ArxivRetriever(top_k_results=3)\n",
    "def multiple_custom_retriever(query:str):\n",
    "    all_docs=[]\n",
    "    wiki=wiki_retriever.invoke(query)\n",
    "    all_docs.append(\"========Wikipedia Information============\")\n",
    "    all_docs.extend(doc.page_content[:500] for doc in wiki)\n",
    "    arxiv=arxiv_retriever.invoke(query)\n",
    "    all_docs.append(\"========Arxiv Information============\")\n",
    "    all_docs.extend(doc.page_content[:500] for doc in arxiv)\n",
    "\n",
    "    return \"\\n\\n\".join(all_docs)\n",
    "\n",
    "custom_retriever=RunnableLambda(multiple_custom_retriever)\n",
    "\n",
    "prompts=PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain=({\"Context\":custom_retriever,\"Question\":RunnablePassthrough()}|prompts|llm|StrOutputParser())\n",
    "query=\"What is Cow?\"\n",
    "result=chain.invoke(query)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d6ca31",
   "metadata": {},
   "source": [
    "Tavily+Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58bb8453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the combined context from Wikipedia and Arxiv, a cow can refer to a female animal of the species cattle that is kept on farms for its milk. It can also refer to the mature female of various other mammals. Additionally, in the context of BDSM subculture, HuCow, people roleplay as dairy cows.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever,TavilySearchAPIRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "prompt_template=\"\"\"\n",
    "Answer the Question Based on both Context from Wikepedia and Arxiv\n",
    "Context:{Context}\n",
    "\n",
    "Question:{Question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "wiki_retriever=WikipediaRetriever(top_k_results=3)\n",
    "tavily_retriever=TavilySearchAPIRetriever(k=3)\n",
    "def multiple_custom_retriever(query:str):\n",
    "    all_docs=[]\n",
    "    wiki=wiki_retriever.invoke(query)\n",
    "    all_docs.append(\"========Wikipedia Information============\")\n",
    "    all_docs.extend(doc.page_content[:500] for doc in wiki)\n",
    "    tavily=tavily_retriever.invoke(query)\n",
    "    all_docs.append(\"========Arxiv Information============\")\n",
    "    all_docs.extend(doc.page_content[:500] for doc in tavily)\n",
    "\n",
    "    return \"\\n\\n\".join(all_docs)\n",
    "\n",
    "custom_retriever=RunnableLambda(multiple_custom_retriever)\n",
    "\n",
    "prompts=PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain=({\"Context\":custom_retriever,\"Question\":RunnablePassthrough()}|prompts|llm|StrOutputParser())\n",
    "query=\"What is Cow?\"\n",
    "result=chain.invoke(query)\n",
    "print(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
