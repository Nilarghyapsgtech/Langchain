{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89fff2a",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2db3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample,EvaluationDataset,evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,ResponseRelevancy,LLMContextPrecisionWithReference,LLMContextRecall,NoiseSensitivity,ContextEntityRecall\n",
    ")\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from ragas.dataset import Dataset\n",
    "from langchain_core.documents import Document\n",
    "import warnings\n",
    "from pydantic import ConfigDict\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b21c9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "def run_async_function(cornjob):\n",
    "    \"\"\"Helper Function To Run Async Functions in Jupyter Notebooks\"\"\"\n",
    "    try:\n",
    "        loop=asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            nest_asyncio.apply()\n",
    "            return loop.run_until_complete(cornjob)\n",
    "        else:\n",
    "            return asyncio.run(cornjob)\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(cornjob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5ce64",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0109764",
   "metadata": {},
   "source": [
    "Faithfullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed395b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context=[\n",
    "    \"LLMs are trained on large text datasets and do not have consciousness or personal beliefs.\"\n",
    "]\n",
    "\n",
    "test_response=\"No, LLMs do not have personal beliefs and only generate text based on learned patterns.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43a3f5",
   "metadata": {},
   "source": [
    "Generate Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "109517aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LLMs do not have personal beliefs.\n",
      "2. LLMs only generate text based on learned patterns.\n"
     ]
    }
   ],
   "source": [
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "    \"\"\"Given the following response,extract all factual claims as a numbered list.\n",
    "    Each claim should be single, verifiable statement\n",
    "    \n",
    "    Response:{test_response}\n",
    "    Extract all Factual Claims\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "\n",
    "result=chain.invoke({\"test_response\":test_response})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a726c",
   "metadata": {},
   "source": [
    "Match Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9147fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'claim': 'LLMs do not have personal beliefs.', 'is_supported': True, 'Explanation': 'SUPPORTED \\n\\nExplanation: The context explicitly states that LLMs do not have personal beliefs as they are trained on large text datasets and do not have consciousness. This aligns with the claim that LLMs do not have personal beliefs.'}, {'claim': 'LLMs only generate text based on learned patterns.', 'is_supported': True, 'Explanation': 'SUPPORTED\\n\\nExplanation: The context clearly states that LLMs do not have consciousness or personal beliefs, indicating that they generate text based on learned patterns and data inputs rather than personal intentions or thoughts. This supports the claim that LLMs only generate text based on learned patterns.'}, {'claim': 'Today is a Sunny day', 'is_supported': False, 'Explanation': 'NOT SUPPORTED\\n\\nExplanation: The context provided about LLMs being trained on text datasets and lacking consciousness has no relevance to the claim about today being a sunny day. The context does not provide any information that would support or relate to the claim.'}]\n"
     ]
    }
   ],
   "source": [
    "claims=[\n",
    "    \"LLMs do not have personal beliefs.\",\n",
    "    \"LLMs only generate text based on learned patterns.\",\n",
    "    \"Today is a Sunny day\"\n",
    "]\n",
    "prompt_template=ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the Claim and Context,verify if the claim is Supported by the context\n",
    "\n",
    "Claim:{Claim}\n",
    "Context:{Context}\n",
    "\n",
    "Answer with:\n",
    " - \"SUPPORTED\" if the context supports the claim\n",
    " - \"NOT SUPPORTED\" if the context doesn't support the claim\n",
    "\n",
    " Also give a Brief Explanation\n",
    " Verdict: \n",
    "\"\"\"\n",
    ")\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "\n",
    "verification_results=[]\n",
    "\n",
    "for claim in claims:\n",
    "    result=chain.invoke(\n",
    "        {\"Claim\":claim,\"Context\":test_context[0]}\n",
    "    )\n",
    "    is_supported=\"SUPPORTED\" in result.upper() and \"NOT SUPPORTED\" not in result.upper()\n",
    "    verification_results.append(\n",
    "        {\n",
    "            \"claim\":claim,\n",
    "            \"is_supported\":is_supported,\n",
    "            \"Explanation\":result\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(verification_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb61684",
   "metadata": {},
   "source": [
    "Calculate Faithfullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e38c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "supported_claims=sum(1 for r in verification_results if r[\"is_supported\"])\n",
    "total_claims=len(verification_results)\n",
    "\n",
    "faithfilness_score=supported_claims/total_claims\n",
    "print(f\"{faithfilness_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9e7de",
   "metadata": {},
   "source": [
    "Ragas Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e69afc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "failthfulness_sample=SingleTurnSample(\n",
    "    user_input=\"Do Large Language Models have personal beliefs?\",\n",
    "    response=test_response,\n",
    "    retrieved_contexts=test_context\n",
    ")\n",
    "\n",
    "faithfulness_metric=Faithfulness(llm=llm)\n",
    "faithfulness_score=run_async_function(faithfulness_metric.single_turn_ascore(failthfulness_sample))\n",
    "print(faithfilness_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c76defe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect Faithfulness (No hallucinations)\n",
      "0.50\n",
      "**************************************************\n",
      "Partial Faithfulness (Some hallucinations)\n",
      "0.00\n",
      "**************************************************\n",
      "Zero Faithfulness (Complete hallucination)\n",
      "0.00\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "faithfulness_examples = [\n",
    "    {\n",
    "        \"name\": \"Perfect Faithfulness (No hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was played on January 15, 1967 at the Los Angeles Memorial Coliseum.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial Faithfulness (Some hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was on January 15, 1967. The Green Bay Packers won 35-10 with Bart Starr as MVP.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Zero Faithfulness (Complete hallucination)\",\n",
    "        \"response\": \"The first Super Bowl was held in Miami in 1970 and attracted over 100,000 spectators.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "faithfulness_metric=Faithfulness(llm=llm)\n",
    "for items in faithfulness_examples:\n",
    "    faithfulness_sample=SingleTurnSample(\n",
    "        user_input=\"Tell me about the first Super Bowl\",\n",
    "        response=items[\"response\"],\n",
    "        retrieved_contexts=items[\"context\"]\n",
    "    )\n",
    "\n",
    "    faithfulness_score=run_async_function(faithfulness_metric.single_turn_ascore(faithfulness_sample))\n",
    "\n",
    "    print(items[\"name\"])\n",
    "    print(f\"{faithfulness_score:.2f}\")\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17ba6a",
   "metadata": {},
   "source": [
    "Answer Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72179ef",
   "metadata": {},
   "source": [
    "Hypothetical Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa10052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "question=\"Do Large Language Models have personal beliefs?\"\n",
    "llm_response=\"No, LLMs do not have personal beliefs and only generate text based on learned patterns.\"\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "From the Answer given generate Hypothecial questions which are Questions to the answer.Strictly ensure that to all the questions generated\n",
    "they should all have the same answer i.e the answer given\n",
    "\n",
    "Answer:{Answer}\n",
    "\n",
    "Construct 3 questions in a numbered manner like\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "\n",
    "result=chain.invoke({\"Answer\":llm_response})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5409eb",
   "metadata": {},
   "source": [
    "Cosine SImilarity Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fe412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "def calculate_cosine_similarity(vec1,vec2):\n",
    "    vec1=np.array(vec1)\n",
    "    vec2=np.array(vec2)\n",
    "    num=np.dot(vec1,vec2)\n",
    "    dem=norm(vec1)*norm(vec2)\n",
    "    return num/dem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8f28f",
   "metadata": {},
   "source": [
    "Calculate Similarities Of the Original Question to the Questions generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a35001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.5985796899687075), np.float64(0.4147325779462339), np.float64(0.46830935807152213)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "generated_questions=[\n",
    "    \"Do LLMs possess personal beliefs when generating text?\",\n",
    "    \"Are LLMs only capable of generating text based on learned patterns?\",\n",
    "    \"Can LLMs form their own personal opinions when generating text?\"\n",
    "]\n",
    "embeddings=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "original_embedding=embeddings.embed_query(question)\n",
    "\n",
    "similarity_index=[]\n",
    "for items in generated_questions:\n",
    "    sim=embeddings.embed_query(items)\n",
    "    cosine=calculate_cosine_similarity(original_embedding,sim)\n",
    "    similarity_index.append(cosine)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7b8f4",
   "metadata": {},
   "source": [
    "Mean of the Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "671c7317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49387387532882115\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.mean(similarity_index)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0787fa",
   "metadata": {},
   "source": [
    "Using the RAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a8831b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5850632477874373\n"
     ]
    }
   ],
   "source": [
    "response_relevancy_sample=SingleTurnSample(\n",
    "    user_input=question,\n",
    "    response=llm_response,\n",
    "    retrieved_contexts=test_context\n",
    ")\n",
    "\n",
    "response_relevancy_metric=ResponseRelevancy(\n",
    "    llm=llm,embeddings=embeddings\n",
    ")\n",
    "\n",
    "response_relevancy_score=run_async_function(response_relevancy_metric.single_turn_ascore(response_relevancy_sample))\n",
    "\n",
    "print(response_relevancy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b8d98",
   "metadata": {},
   "source": [
    "Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde68d8",
   "metadata": {},
   "source": [
    "Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9eeffb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is an LLM?\"\n",
    "response=\"An LLM is an AI model trained on large text datasets to generate and understand human language.\"\n",
    "\n",
    "chunks=[\n",
    "    \"The Eiffel Tower is located in Paris and was completed in 1889.\",\n",
    "    \"Python lists are mutable data structures used to store collections of items.\",\n",
    "    \"Large Language Models are trained on massive text data and can generate human-like responses.\",\n",
    "    \"LLMs are commonly used for tasks like question answering and text generation.\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e7daee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "For the Given Question and Answer verify if the retrieved chunk is relevant or not\n",
    "Question:{Question},\n",
    "Answer:{Answer}\n",
    "Chunk:{Chunk}\n",
    "If the chunk is relevant give RELEVANT else give IRRELEVANT\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "relevancy_matrix=[]\n",
    "for chunk in chunks:\n",
    "    result=chain.invoke(\n",
    "        {\n",
    "            \"Question\":query,\n",
    "            \"Answer\":response,\n",
    "            \"Chunk\":chunk\n",
    "        }\n",
    "    )\n",
    "    is_relevant=\"RELEVANT\" in result.upper() and \"IRRELEVANT\" not in result.upper()\n",
    "    relevancy_matrix.append(is_relevant)\n",
    "\n",
    "print(relevancy_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab44fb5",
   "metadata": {},
   "source": [
    "Precision count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50b18b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41666666666666663\n"
     ]
    }
   ],
   "source": [
    "\n",
    "precision_good=[]\n",
    "relevant_count=0\n",
    "for k,relevancy in enumerate(relevancy_matrix,1):\n",
    "    if relevancy:\n",
    "        relevant_count+=1\n",
    "        precision_at_k=relevant_count/k\n",
    "        precision_good.append(precision_at_k)\n",
    "\n",
    "\n",
    "\n",
    "total_relevancy_matrix=sum(relevancy_matrix)\n",
    "\n",
    "total_context_precision=sum(precision_good)/sum(relevancy_matrix) if total_relevancy_matrix>0 else 0\n",
    "print(total_context_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5eb572",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9443e1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999999995\n",
      "0.4166666666458333\n"
     ]
    }
   ],
   "source": [
    "bad_sample=[\n",
    "    \"The Eiffel Tower is located in Paris and was completed in 1889.\",\n",
    "    \"Python lists are mutable data structures used to store collections of items.\",\n",
    "    \"Large Language Models are trained on massive text data and can generate human-like responses.\",\n",
    "    \"LLMs are commonly used for tasks like question answering and text generation.\"\n",
    "]\n",
    "\n",
    "good_sample=[\n",
    "    \"Large Language Models are trained on massive text data and can generate human-like responses.\",\n",
    "    \"LLMs are commonly used for tasks like question answering and text generation.\",\n",
    "    \"The Eiffel Tower is located in Paris and was completed in 1889.\",\n",
    "    \"Python lists are mutable data structures used to store collections of items.\"\n",
    "]\n",
    "\n",
    "context_precision_sample_good=SingleTurnSample(\n",
    "    user_input=query,\n",
    "    reference=response,\n",
    "    retrieved_contexts=good_sample\n",
    ")\n",
    "context_precision_sample_bad=SingleTurnSample(\n",
    "    user_input=query,\n",
    "    reference=response,\n",
    "    retrieved_contexts=bad_sample\n",
    ")\n",
    "\n",
    "context_precision_metric=LLMContextPrecisionWithReference(llm=llm)\n",
    "good_result=run_async_function(context_precision_metric.single_turn_ascore(context_precision_sample_good))\n",
    "bad_result=run_async_function(context_precision_metric.single_turn_ascore(context_precision_sample_bad))\n",
    "print(good_result)\n",
    "print(bad_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537c895",
   "metadata": {},
   "source": [
    "Conext Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1904bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim is The Eifel Tower is Located in Paris and is True\n",
      "Claim is It was buit in 1889 and is True\n",
      "Claim is It is 330 meters tall and is False\n"
     ]
    }
   ],
   "source": [
    "query=\"Tell me about Eiffel Tower\"\n",
    "recall_reference=\"The Eifel Tower is Located in Paris.It was buit in 1889.It is 330 meters tall\"\n",
    "\n",
    "recall_context=[\n",
    "    \"The Eifel Tower is a Landmark Located in Paris,France\",\n",
    "    \"The Tower was completed in 1889or the World's Fair\"\n",
    "]\n",
    "recall_claims=[\n",
    "    \"The Eifel Tower is Located in Paris\",\n",
    "    \"It was buit in 1889\",\n",
    "    \"It is 330 meters tall\"\n",
    "]\n",
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Check if the the Following Claims can be attributed to the Context\n",
    "    Claim:{Claim}\n",
    "    Context:{Context}\n",
    "\n",
    "    if the Claim is supported by the Context gives \"YES\" if not then gine \"NO\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "combined_context=\"\\n\".join(recall_context)\n",
    "claims=[]\n",
    "for claim in recall_claims:\n",
    "    result=chain.invoke({\n",
    "        \"Claim\":claim,\n",
    "        \"Context\":combined_context\n",
    "    })\n",
    "    is_supported=\"YES\" in result.upper() and \"NO\" not in result.upper()\n",
    "    claims.append(is_supported)\n",
    "    print(f\"Claim is {claim} and is {is_supported}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672afc16",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b9e0d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "context_recall_sample=SingleTurnSample(\n",
    "    user_input=query,\n",
    "    reference=recall_reference,\n",
    "    retrieved_contexts=recall_context\n",
    ")\n",
    "\n",
    "context_recall_metric=LLMContextRecall(llm=llm)\n",
    "context_recall_score=run_async_function(context_recall_metric.single_turn_ascore(context_recall_sample))\n",
    "\n",
    "print(context_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096c806",
   "metadata": {},
   "source": [
    "Context Entity Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f6c7fccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: William Shakespeare\n",
      "LOCATION: England\n",
      "LOCATION: 16th\n",
      "LOCATION: 17th century\n",
      "PLAY: Hamlet\n",
      "PLAY: Romeo and Juliet\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "PERSON: William Shakespeare\n",
      "LOCATION: English\n",
      "DATE: N/A\n",
      "WORK_OF_ART: literature\n"
     ]
    }
   ],
   "source": [
    "entity_reference = \"William Shakespeare wrote many famous plays such as Hamlet and Romeo and Juliet in England during the late 16th and early 17th century.\"\n",
    "entity_context = [\n",
    "    \"William Shakespeare was an English playwright known for his influential works in literature.\"\n",
    "]\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Extract all named entities from the following Text.\n",
    "Include: PERSON,NAME,DATE,LOCATION and other proper nouns\n",
    "Text:{Text}\n",
    "List each entity on a new line with its type:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template|llm|StrOutputParser()\n",
    "reference_result=chain.invoke({\"Text\":entity_reference})\n",
    "context_result=chain.invoke({\"Text\":entity_context[0]})\n",
    "print(reference_result)\n",
    "print('@'*50)\n",
    "print(context_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d7164b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "reference_entities = {\n",
    "    \"William Shakespeare\": \"PERSON\",\n",
    "    \"Hamlet\": \"PERSON\",\n",
    "    \"Romeo\": \"PERSON\",\n",
    "    \"Juliet\": \"PERSON\",\n",
    "    \"England\": \"LOCATION\",\n",
    "    \"late 16th and early 17th century\": \"DATE\"\n",
    "}\n",
    "\n",
    "context_entities = {\n",
    "    \"William Shakespeare\": \"PERSON\",\n",
    "    \"English\": \"LOCATION\",\n",
    "    \"playwright\": \"PERSON\"\n",
    "}\n",
    "\n",
    "\n",
    "common_entities_reference_context=reference_entities.keys() and context_entities.keys()\n",
    "common_entities_reference_context_size=len(common_entities_reference_context)\n",
    "reference_entities_size=len(reference_entities.keys())\n",
    "context_entity_recall=common_entities_reference_context_size/reference_entities_size\n",
    "\n",
    "print(context_entity_recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6725a",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d6ac6f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999996\n"
     ]
    }
   ],
   "source": [
    "context_entity_recall_sample=SingleTurnSample(\n",
    "    reference=entity_reference,\n",
    "    retrieved_contexts=entity_context\n",
    ")\n",
    "\n",
    "context_entity_recall_metric=ContextEntityRecall(llm=llm)\n",
    "context_entity_recall=run_async_function(context_entity_recall_metric.single_turn_ascore(context_entity_recall_sample))\n",
    "print(context_entity_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9675b",
   "metadata": {},
   "source": [
    "Noise Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4075a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_question = \"What is LIC known for?\"\n",
    "noise_response = \"LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.\"\n",
    "noise_reference = \"LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.\"\n",
    "\n",
    "noise_contexts = [\n",
    "    \"LIC was established in 1956 following nationalization.\",      \n",
    "    \"LIC is the largest insurance company with huge investments.\",     \n",
    "    \"LIC manages substantial funds for financial stability.\",           \n",
    "    \"The Indian economy is one of the fastest-growing economies...\"     \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b450383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContextLIC was established in 1956 following nationalization. True\n",
      "ContextLIC is the largest insurance company with huge investments. TRUE\n",
      "ContextLIC manages substantial funds for financial stability. TRUE\n",
      "ContextThe Indian economy is one of the fastest-growing economies... FALSE\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "For the reference given Below verify if the a given context directly support the reference or not\n",
    "\n",
    "reference:{reference}\n",
    "context:{context}\n",
    "\n",
    "return TRUE if the context directly supports the reference or FALSE if it doesnot directly supports the reference\n",
    "\"\"\"\n",
    ")\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "for context in noise_contexts:\n",
    "    result=chain.invoke(\n",
    "        {\n",
    "            \"reference\":noise_reference,\n",
    "            \"context\":context\n",
    "        }\n",
    "    )\n",
    "    print(f\"Context{context} {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c4fec3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "response_claims = [\n",
    "    (\"LIC is the largest insurance company in India\", True, \"Matches reference\"),\n",
    "    (\"LIC is known for its vast portfolio\", True, \"Matches reference (portfolio)\"),\n",
    "    (\"LIC contributes to financial stability\", False, \"NOT in reference - possible hallucination from noise!\")\n",
    "]\n",
    "\n",
    "incorrect_count = 0\n",
    "for claim in response_claims:\n",
    "    if not claim[1]:\n",
    "        incorrect_count+=1\n",
    "\n",
    "noise=incorrect_count/len(response_claims)\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa7f31",
   "metadata": {},
   "source": [
    "RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f710da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "noise_sample=SingleTurnSample(\n",
    "    user_input=\"What is LIC known for?\",\n",
    "    response=\"LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.\",\n",
    "    reference=\"LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.\",\n",
    "    retrieved_contexts=noise_contexts\n",
    ")\n",
    "\n",
    "noise_metric=NoiseSensitivity(llm=llm)\n",
    "noise=run_async_function(noise_metric.single_turn_ascore(noise_sample))\n",
    "\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d0524",
   "metadata": {},
   "source": [
    "Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "026ab728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 17 CloudFlow documentation documents\n",
      "\n",
      "Document breakdown by category:\n",
      "  - Architecture: 3 documents\n",
      "  - Api: 4 documents\n",
      "  - Security: 2 documents\n",
      "  - Pricing: 2 documents\n",
      "  - Best_Practices: 3 documents\n",
      "  - Troubleshooting: 3 documents\n"
     ]
    }
   ],
   "source": [
    "cloudflow_docs = [\n",
    "    # ============================================================================\n",
    "    # ARCHITECTURE DOCUMENTS (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Architecture Overview\n",
    "\n",
    "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\n",
    "\n",
    "The API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\n",
    "\n",
    "The Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\n",
    "\n",
    "The Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.\n",
    "\n",
    "CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.\"\"\",\n",
    "        metadata={\"source\": \"architecture_overview\", \"topic\": \"architecture\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Scaling Mechanisms\n",
    "\n",
    "CloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\n",
    "\n",
    "Horizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\n",
    "\n",
    "Vertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\n",
    "\n",
    "The platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling.\n",
    "\n",
    "Load balancing distributes traffic across all available pods using a weighted round-robin algorithm. Health checks run every 10 seconds, and unhealthy pods are automatically removed from the rotation within 30 seconds.\"\"\",\n",
    "        metadata={\"source\": \"scaling_guide\", \"topic\": \"architecture\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow System Components\n",
    "\n",
    "CloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\n",
    "\n",
    "The Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\n",
    "\n",
    "The Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\n",
    "\n",
    "The Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\n",
    "\n",
    "The Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\n",
    "\n",
    "The Message Queue system, based on Apache Kafka, handles asynchronous communication between services with guaranteed message delivery and ordering.\"\"\",\n",
    "        metadata={\"source\": \"system_components\", \"topic\": \"architecture\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # API DOCUMENTATION (4)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Authentication\n",
    "\n",
    "CloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\n",
    "\n",
    "OAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\n",
    "\n",
    "API Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\n",
    "\n",
    "To authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.\n",
    "\n",
    "API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.\"\"\",\n",
    "        metadata={\"source\": \"api_authentication\", \"topic\": \"api\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow REST API Endpoints\n",
    "\n",
    "CloudFlow provides a comprehensive REST API with endpoints organized by resource type. All endpoints follow RESTful conventions and return JSON responses.\n",
    "\n",
    "Base URL: https://api.cloudflow.io/v1\n",
    "\n",
    "Resources endpoint: GET /api/v1/resources - List all resources with pagination (max 100 per page). Supports filtering by type, status, and creation date.\n",
    "\n",
    "Resource creation: POST /api/v1/resources - Create a new resource. Required fields: name (string), type (string), config (object). Returns 201 Created on success.\n",
    "\n",
    "Resource details: GET /api/v1/resources/{id} - Retrieve detailed information about a specific resource by ID.\n",
    "\n",
    "Resource update: PUT /api/v1/resources/{id} - Update an existing resource. Supports partial updates with PATCH /api/v1/resources/{id}.\n",
    "\n",
    "Resource deletion: DELETE /api/v1/resources/{id} - Delete a resource. Returns 204 No Content on success. Deleted resources are soft-deleted and can be recovered within 30 days.\n",
    "\n",
    "All list endpoints support query parameters: limit (default: 25, max: 100), offset (default: 0), sort (default: created_at), order (asc|desc).\"\"\",\n",
    "        metadata={\"source\": \"api_endpoints\", \"topic\": \"api\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Rate Limiting\n",
    "\n",
    "CloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\n",
    "\n",
    "Standard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\n",
    "\n",
    "Premium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\n",
    "\n",
    "Enterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\n",
    "\n",
    "Rate limit headers are included in every response:\n",
    "- X-RateLimit-Limit: Maximum requests per hour\n",
    "- X-RateLimit-Remaining: Remaining requests in current window\n",
    "- X-RateLimit-Reset: Unix timestamp when the limit resets\n",
    "\n",
    "When rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.\n",
    "\n",
    "OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.\"\"\",\n",
    "        metadata={\"source\": \"api_rate_limits\", \"topic\": \"api\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Error Codes\n",
    "\n",
    "CloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\n",
    "\n",
    "Authentication Errors:\n",
    "- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\n",
    "- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\n",
    "\n",
    "Client Errors:\n",
    "- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\n",
    "- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\n",
    "- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\n",
    "- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\n",
    "- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.\n",
    "\n",
    "Server Errors:\n",
    "- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\n",
    "- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\n",
    "- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\n",
    "\n",
    "Error Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}\"\"\",\n",
    "        metadata={\"source\": \"api_error_codes\", \"topic\": \"api\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SECURITY DOCUMENTATION (2)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Security Features\n",
    "\n",
    "Security is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\n",
    "\n",
    "Encryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\n",
    "\n",
    "Network Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\n",
    "\n",
    "Access Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.\n",
    "\n",
    "Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\n",
    "\n",
    "Vulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours.\"\"\",\n",
    "        metadata={\"source\": \"security_features\", \"topic\": \"security\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Compliance Standards\n",
    "\n",
    "CloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\n",
    "\n",
    "SOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\n",
    "\n",
    "GDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\n",
    "\n",
    "HIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\n",
    "\n",
    "ISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\n",
    "\n",
    "PCI DSS: For customers processing payment card data, CloudFlow provides PCI DSS Level 1 certified infrastructure. However, we recommend using dedicated payment processors rather than storing card data.\n",
    "\n",
    "Data Residency: CloudFlow supports data residency in US, EU, UK, and APAC regions to meet local regulatory requirements.\"\"\",\n",
    "        metadata={\"source\": \"compliance_standards\", \"topic\": \"security\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PRICING DOCUMENTATION (2)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Pricing Tiers\n",
    "\n",
    "CloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\n",
    "\n",
    "Standard Tier ($99/month):\n",
    "- 1,000 API requests per hour\n",
    "- 100 GB storage included\n",
    "- 10 GB bandwidth per month\n",
    "- Community support via forums\n",
    "- 99.9% uptime SLA\n",
    "- Up to 5 team members\n",
    "\n",
    "Premium Tier ($499/month):\n",
    "- 10,000 API requests per hour\n",
    "- 1 TB storage included\n",
    "- 100 GB bandwidth per month\n",
    "- Email support with 24-hour response time\n",
    "- 99.95% uptime SLA\n",
    "- Up to 25 team members\n",
    "- Advanced monitoring and alerting\n",
    "- Custom domain support\n",
    "\n",
    "Enterprise Tier (Custom pricing):\n",
    "- Custom API rate limits (100,000+ requests/hour)\n",
    "- Unlimited storage and bandwidth\n",
    "- 24/7 phone and email support with 1-hour response time\n",
    "- 99.99% uptime SLA with service credits\n",
    "- Unlimited team members\n",
    "- Dedicated account manager\n",
    "- Custom integrations and professional services\n",
    "- Private cloud deployment options\n",
    "\n",
    "All tiers include: SSL certificates, daily backups, API access, and dashboard analytics. Annual billing provides 15% discount.\"\"\",\n",
    "        metadata={\"source\": \"pricing_tiers\", \"topic\": \"pricing\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Billing Information\n",
    "\n",
    "Understanding CloudFlow's billing model helps you manage costs effectively and avoid unexpected charges.\n",
    "\n",
    "Billing Cycle: Subscriptions are billed monthly on the date you signed up. Annual subscriptions are billed upfront with a 15% discount. Billing date can be changed once per year.\n",
    "\n",
    "Usage-Based Charges: Beyond included quotas, additional usage is billed at:\n",
    "- API requests: $0.01 per 1,000 requests\n",
    "- Storage: $0.10 per GB per month\n",
    "- Bandwidth: $0.08 per GB\n",
    "- Backup retention (beyond 30 days): $0.05 per GB per month\n",
    "\n",
    "Payment Methods: CloudFlow accepts credit cards (Visa, Mastercard, Amex), ACH transfers (US only), and wire transfers for invoices over $1,000. Cryptocurrency payments available for annual plans.\n",
    "\n",
    "Invoicing: Invoices are emailed on the billing date and available in the dashboard. Enterprise customers receive consolidated monthly invoices with 30-day payment terms.\n",
    "\n",
    "Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\n",
    "\n",
    "Free Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.\"\"\",\n",
    "        metadata={\"source\": \"billing_info\", \"topic\": \"pricing\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # BEST PRACTICES DOCUMENTATION (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Performance Optimization\n",
    "\n",
    "Following these best practices will help you achieve optimal performance from your CloudFlow applications.\n",
    "\n",
    "Caching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\n",
    "\n",
    "Request Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\n",
    "\n",
    "Connection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\n",
    "\n",
    "Query Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally.\n",
    "\n",
    "Asynchronous Processing: For long-running operations, use CloudFlow's async API endpoints. Poll for results using the returned job_id rather than blocking on the initial request.\n",
    "\n",
    "CDN Usage: Serve static assets through CloudFlow's global CDN with 150+ edge locations. Configure appropriate cache headers for optimal performance: max-age=3600 for semi-static content.\"\"\",\n",
    "        metadata={\"source\": \"performance_optimization\", \"topic\": \"best_practices\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Monitoring and Observability\n",
    "\n",
    "Effective monitoring ensures your CloudFlow applications remain healthy and performant.\n",
    "\n",
    "Metrics Collection: CloudFlow automatically collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization. Access metrics via the dashboard or Metrics API at /api/v1/metrics.\n",
    "\n",
    "Custom Metrics: Send custom application metrics using the StatsD protocol. CloudFlow aggregates custom metrics every 60 seconds and retains them for 90 days.\n",
    "\n",
    "Alerting: Configure alerts for critical conditions like error rate >5%, latency >500ms, or approaching rate limits. CloudFlow supports alerting via email, SMS, Slack, PagerDuty, and webhooks.\n",
    "\n",
    "Distributed Tracing: Enable distributed tracing to track requests across services. CloudFlow supports OpenTelemetry and provides trace visualization in the dashboard. Sample rate: 10% of requests (configurable up to 100%).\n",
    "\n",
    "Log Management: CloudFlow retains logs for 7 days by default (30 days for Premium, 90 days for Enterprise). Use structured logging with JSON format for better searchability. Maximum log line length: 32KB.\n",
    "\n",
    "Dashboard Widgets: Create custom dashboards with real-time metrics, SLA compliance, and cost tracking. Share dashboards with team members or embed in external tools using iframe integration.\"\"\",\n",
    "        metadata={\"source\": \"monitoring_observability\", \"topic\": \"best_practices\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Disaster Recovery\n",
    "\n",
    "CloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\n",
    "\n",
    "Backup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\n",
    "\n",
    "Point-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\n",
    "\n",
    "Multi-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\n",
    "\n",
    "Backup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.\n",
    "\n",
    "Export Capabilities: Export your data anytime in JSON, CSV, or Parquet format. Full exports are available via the /api/v1/export endpoint. Large exports (>10 GB) are delivered to your S3 bucket.\n",
    "\n",
    "RTO and RPO: CloudFlow guarantees Recovery Time Objective (RTO) of 4 hours and Recovery Point Objective (RPO) of 1 hour for Enterprise tier. Contact support to initiate disaster recovery procedures.\"\"\",\n",
    "        metadata={\"source\": \"disaster_recovery\", \"topic\": \"best_practices\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # TROUBLESHOOTING DOCUMENTATION (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"Common CloudFlow Errors and Solutions\n",
    "\n",
    "This guide covers the most common errors encountered when using CloudFlow and their solutions.\n",
    "\n",
    "Error: \"Invalid API Key\" (401)\n",
    "Solution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\n",
    "\n",
    "Error: \"Rate Limit Exceeded\" (429)\n",
    "Solution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\n",
    "\n",
    "Error: \"Resource Not Found\" (404)\n",
    "Solution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).\n",
    "\n",
    "Error: \"Timeout\" (504)\n",
    "Solution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\n",
    "\n",
    "Error: \"Validation Error\" (422)\n",
    "Solution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.\"\"\",\n",
    "        metadata={\"source\": \"common_errors\", \"topic\": \"troubleshooting\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Debugging Guide\n",
    "\n",
    "When troubleshooting issues with CloudFlow, follow this systematic debugging approach.\n",
    "\n",
    "Step 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\n",
    "\n",
    "Step 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\n",
    "\n",
    "Step 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\n",
    "\n",
    "Step 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources\n",
    "\n",
    "Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\n",
    "\n",
    "Step 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\n",
    "\n",
    "Step 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\"\"\",\n",
    "        metadata={\"source\": \"debugging_guide\", \"topic\": \"troubleshooting\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Support Escalation Process\n",
    "\n",
    "Understanding CloudFlow's support escalation process ensures your issues are resolved efficiently.\n",
    "\n",
    "Support Channels:\n",
    "- Community Forums (All tiers): community.cloudflow.io - Best for general questions, feature requests, and sharing knowledge\n",
    "- Email Support (Premium & Enterprise): support@cloudflow.io - Include account ID and request ID in subject line\n",
    "- Phone Support (Enterprise only): +1-888-CLOUDFLOW - Available 24/7 for critical issues\n",
    "- Slack Channel (Enterprise only): Direct access to engineering team\n",
    "\n",
    "Issue Severity Levels:\n",
    "- P0 (Critical): Complete service outage affecting production. Response time: 1 hour for Enterprise, 4 hours for Premium\n",
    "- P1 (High): Major functionality impaired but workarounds available. Response time: 4 hours for Enterprise, 8 hours for Premium\n",
    "- P2 (Medium): Minor functionality issues with workarounds. Response time: 24 hours\n",
    "- P3 (Low): Questions, feature requests, documentation issues. Response time: 48 hours\n",
    "\n",
    "Escalation Path: If your issue isn't resolved within SLA, it automatically escalates to the next support tier. Enterprise customers can request immediate escalation to engineering team.\n",
    "\n",
    "Required Information: Include account ID, request ID, error messages, timestamps, steps to reproduce, and expected vs actual behavior. Screenshots and API logs are helpful.\"\"\",\n",
    "        metadata={\"source\": \"support_escalation\", \"topic\": \"troubleshooting\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(cloudflow_docs)} CloudFlow documentation documents\")\n",
    "print(\"\\nDocument breakdown by category:\")\n",
    "for topic in [\"architecture\", \"api\", \"security\", \"pricing\", \"best_practices\", \"troubleshooting\"]:\n",
    "    count = len([doc for doc in cloudflow_docs if doc.metadata[\"topic\"] == topic])\n",
    "    print(f\"  - {topic.title()}: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7346b",
   "metadata": {},
   "source": [
    "Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a307311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=128,\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "         \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "split=splitter.split_documents(cloudflow_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a425f",
   "metadata": {},
   "source": [
    "Embedding and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d56f4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "vectorstore=FAISS.from_documents(split,embeddings)\n",
    "\n",
    "vectorstore_path=\"FAISS\"\n",
    "\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65251d8",
   "metadata": {},
   "source": [
    "Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9370c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.\n"
     ]
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\":5\n",
    "    }\n",
    ")\n",
    "\n",
    "query=\"What is CloudFlow's uptime SLA\"\n",
    "\n",
    "result=retriever.invoke(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0477d",
   "metadata": {},
   "source": [
    "Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b98d380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I am an AI assistant created by OpenAI, so I don't have feelings or emotions like a human. But I am here to help and assist you with any questions or tasks you may have. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 13, 'total_tokens': 60, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D0O30LswN0o5nSpXs8Z2cB0PFU56L', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bdfb6-1049-73e1-8ed2-183aa4c7e78a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 13, 'output_tokens': 47, 'total_tokens': 60, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "llm=ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    max_completion_tokens=500                   # Reasonable for QA system              \n",
    ")\n",
    "\n",
    "test_response=llm.invoke(\"How are you openai guy\")\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043e47a",
   "metadata": {},
   "source": [
    "Prompt Template and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dcd801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "prompt_template =ChatPromptTemplate.from_template(\"\"\"You are a helpful assistant for CloudFlow Platform documentation.\n",
    "Answer the question based on the following context. If you cannot answer based on\n",
    "the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "Be concise and accurate. Include specific details like numbers, limits, and technical\n",
    "specifications when available in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "format_docs=RunnableLambda(format_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94feaa94",
   "metadata": {},
   "source": [
    "LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be267dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: What is CloudFlow's uptime SLA?\n",
      "Answer: CloudFlow's uptime SLA is guaranteed at 99.99% across availability zones.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 2: How do I authenticate with CloudFlow APIs?\n",
      "Answer: To authenticate with CloudFlow APIs, you can use either OAuth 2.0 or API Keys. For OAuth 2.0, direct users to the authorization endpoint with your client_id and redirect_uri parameters. Access tokens are valid for 1 hour, and refresh tokens are valid for 30 days. For API Keys, include your key in the Authorization header with the format \"Authorization: Bearer YOUR_API_KEY\". Ensure all requests are made over HTTPS to avoid a 403 error.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 3: What are the pricing tiers?\n",
      "Answer: The pricing tiers for CloudFlow are Standard Tier ($99/month), Premium Tier ($499/month), and Enterprise Tier (Custom pricing).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ“ RAG pipeline is working correctly!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    {'context':retriever|format_docs,'question':RunnablePassthrough()}|prompt_template|llm|StrOutputParser()\n",
    ")\n",
    "\n",
    "test_questions = [\n",
    "    \"What is CloudFlow's uptime SLA?\",\n",
    "    \"How do I authenticate with CloudFlow APIs?\",\n",
    "    \"What are the pricing tiers?\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    answer = chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nâœ“ RAG pipeline is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bae1bd",
   "metadata": {},
   "source": [
    "Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa10497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 18 test questions with ground truth answers\n",
      "\n",
      "Question breakdown by category:\n",
      "  - Simple Factual: 5 questions\n",
      "  - Multi-Fact: 4 questions\n",
      "  - Procedural: 3 questions\n",
      "  - Comparison: 2 questions\n",
      "  - Troubleshooting: 2 questions\n",
      "  - Edge Cases: 2 questions\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    # ========== SIMPLE FACTUAL (5) ==========\n",
    "    {\n",
    "        \"question\": \"What is CloudFlow's uptime SLA?\",\n",
    "        \"ground_truth\": \"CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What authentication protocol does CloudFlow use?\",\n",
    "        \"ground_truth\": \"CloudFlow uses OAuth 2.0 for user-facing applications and API keys for server-to-server communication.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the service mesh technology used by CloudFlow?\",\n",
    "        \"ground_truth\": \"CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What compliance standards does CloudFlow support?\",\n",
    "        \"ground_truth\": \"CloudFlow supports SOC 2 Type II, GDPR, HIPAA, ISO 27001, and PCI DSS Level 1 compliance standards.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long are CloudFlow audit logs retained?\",\n",
    "        \"ground_truth\": \"CloudFlow audit logs are immutable and retained for 2 years.\"\n",
    "    },\n",
    "    \n",
    "    # ========== MULTI-FACT (4) ==========\n",
    "    {\n",
    "        \"question\": \"What are the three main layers of CloudFlow architecture?\",\n",
    "        \"ground_truth\": \"The three main layers are: API Gateway layer (handles authentication and routing), Service Mesh layer (orchestrates microservices), and Data Storage layer (distributed database with replication).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are CloudFlow's pricing tiers and their API rate limits?\",\n",
    "        \"ground_truth\": \"Standard tier costs $99/month with 1,000 requests/hour, Premium tier costs $499/month with 10,000 requests/hour, and Enterprise tier has custom pricing with 100,000+ requests/hour.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What HTTP status codes indicate authentication failures in CloudFlow API?\",\n",
    "        \"ground_truth\": \"401 Unauthorized indicates missing or invalid API key, and 403 Forbidden indicates valid API key but insufficient permissions.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What auto-scaling metrics does CloudFlow monitor?\",\n",
    "        \"ground_truth\": \"CloudFlow monitors CPU utilization (target 70%), memory usage (target 80%), and triggers scaling when thresholds are exceeded for more than 3 consecutive minutes.\"\n",
    "    },\n",
    "    \n",
    "    # ========== PROCEDURAL (3) ==========\n",
    "    {\n",
    "        \"question\": \"How do I authenticate with CloudFlow APIs using an API key?\",\n",
    "        \"ground_truth\": \"Include your API key in the Authorization header as 'Authorization: Bearer YOUR_API_KEY'. All requests must be made over HTTPS, and API keys have the format 'cf_live_' followed by 32 alphanumeric characters.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I handle rate limit errors in CloudFlow?\",\n",
    "        \"ground_truth\": \"When you receive a 429 error, implement exponential backoff in retry logic, check the X-RateLimit-Reset header to know when limits reset, and use the Retry-After header to determine wait time (1s, then 2s, then 4s, etc.).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What steps should I follow to optimize CloudFlow API performance?\",\n",
    "        \"ground_truth\": \"Implement caching with Redis (TTL 5-60 minutes), batch multiple operations into single API calls, use pagination for large result sets (50-100 items), enable request compression with gzip, and maintain a connection pool with 5-10 concurrent connections.\"\n",
    "    },\n",
    "    \n",
    "    # ========== COMPARISON (2) ==========\n",
    "    {\n",
    "        \"question\": \"What's the difference between Standard and Premium tier rate limits?\",\n",
    "        \"ground_truth\": \"Standard tier allows 1,000 requests per hour with 100 requests per minute burst, while Premium tier allows 10,000 requests per hour with 500 requests per minute burst. Premium also includes priority request processing.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does OAuth 2.0 authentication differ from API key authentication in CloudFlow?\",\n",
    "        \"ground_truth\": \"OAuth 2.0 is recommended for user-facing applications with access tokens valid for 1 hour and provides the Authorization Code flow, while API keys are ideal for server-to-server communication, never expire unless revoked, and have a simpler implementation.\"\n",
    "    },\n",
    "    \n",
    "    # ========== TROUBLESHOOTING (2) ==========\n",
    "    {\n",
    "        \"question\": \"What should I do if I receive a 504 timeout error?\",\n",
    "        \"ground_truth\": \"Increase client timeout to at least 30 seconds, use async endpoints for long-running operations and poll for results, and check the CloudFlow status page for any service degradation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I debug slow API response times in CloudFlow?\",\n",
    "        \"ground_truth\": \"Add X-CloudFlow-Debug: true header to requests for detailed debug information, review API logs in the dashboard under Analytics > API Logs, test with curl commands, and verify network connectivity to *.cloudflow.io on port 443.\"\n",
    "    },\n",
    "    \n",
    "    # ========== EDGE CASES (2) ==========\n",
    "    {\n",
    "        \"question\": \"What happens if I use an expired OAuth token?\",\n",
    "        \"ground_truth\": \"If you use an expired access token, you'll receive a 401 Unauthorized error. You should use your refresh token to obtain a new access token. Access tokens are valid for 1 hour and refresh tokens are valid for 30 days.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does CloudFlow support blockchain integration?\",\n",
    "        \"ground_truth\": \"I don't have enough information to answer that question.\"  # Tests 'I don't know' handling\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(test_cases)} test questions with ground truth answers\\n\")\n",
    "print(\"Question breakdown by category:\")\n",
    "print(\"  - Simple Factual: 5 questions\")\n",
    "print(\"  - Multi-Fact: 4 questions\")\n",
    "print(\"  - Procedural: 3 questions\")\n",
    "print(\"  - Comparison: 2 questions\")\n",
    "print(\"  - Troubleshooting: 2 questions\")\n",
    "print(\"  - Edge Cases: 2 questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389abf72",
   "metadata": {},
   "source": [
    "RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292296f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': [\"What is CloudFlow's uptime SLA?\", 'What authentication protocol does CloudFlow use?', 'What is the service mesh technology used by CloudFlow?', 'What compliance standards does CloudFlow support?', 'How long are CloudFlow audit logs retained?', 'What are the three main layers of CloudFlow architecture?', \"What are CloudFlow's pricing tiers and their API rate limits?\", 'What HTTP status codes indicate authentication failures in CloudFlow API?', 'What auto-scaling metrics does CloudFlow monitor?', 'How do I authenticate with CloudFlow APIs using an API key?', 'How do I handle rate limit errors in CloudFlow?', 'What steps should I follow to optimize CloudFlow API performance?', \"What's the difference between Standard and Premium tier rate limits?\", 'How does OAuth 2.0 authentication differ from API key authentication in CloudFlow?', 'What should I do if I receive a 504 timeout error?', 'How do I debug slow API response times in CloudFlow?', 'What happens if I use an expired OAuth token?', 'Does CloudFlow support blockchain integration?'], 'response': ['CloudFlow guarantees a 99.99% uptime SLA with triple redundancy across availability zones for the Enterprise Tier. The Standard Tier has a 99.9% uptime SLA, and the Premium Tier has a 99.95% uptime SLA.', 'CloudFlow uses two authentication methods: OAuth 2.0 and API Keys. OAuth 2.0 is recommended for user-facing applications, while API Keys are ideal for server-to-server communication and background jobs.', 'The service mesh technology used by CloudFlow is powered by Istio.', \"CloudFlow supports the following compliance standards:\\n\\n1. **SOC 2 Type II**: Certified, demonstrating commitment to security, availability, and confidentiality.\\n2. **GDPR Compliance**: Fully compliant with the EU's General Data Protection Regulation, supporting data residency, right to erasure, data portability, and providing Data Processing Agreements (DPA).\\n3. **HIPAA**: Offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA), including enhanced audit logging, encrypted backups, and strict access controls.\\n4. **ISO 27001**: Certified to ISO 27001:2013 standards for information security management, with annual recertification audits.\", 'CloudFlow audit logs are retained for 2 years.', 'The three main layers of CloudFlow architecture are:\\n\\n1. **API Gateway Layer**: Handles all incoming requests, provides OAuth 2.0 authentication, load balancing, SSL termination, and request routing.\\n\\n2. **Service Mesh Layer**: Orchestrates communication between microservices, providing service discovery, health checking, and automatic failover, using Kubernetes for container orchestration.\\n\\n3. **Data Storage Layer**: Implements a distributed database system with automatic replication across three availability zones, ensuring data durability and supporting horizontal scaling.', 'CloudFlow offers three pricing tiers:\\n\\n1. **Standard Tier ($99/month)**:\\n   - API Rate Limit: 1,000 requests per hour\\n   - Burst Capacity: Up to 100 requests per minute\\n\\n2. **Premium Tier ($499/month)**:\\n   - API Rate Limit: 10,000 requests per hour\\n   - Burst Capacity: Up to 500 requests per minute\\n   - Includes priority request processing\\n\\n3. **Enterprise Tier (Custom pricing)**:\\n   - Custom API Rate Limits: Typically starts at 100,000 requests per hour\\n   - Dedicated infrastructure for higher usage patterns\\n\\nEach tier has specific features and support options tailored to different user needs.', 'The HTTP status codes that indicate authentication failures in CloudFlow API are:\\n\\n- **401 Unauthorized**: Missing or invalid API key. Check the Authorization header.\\n- **403 Forbidden**: Valid API key but insufficient permissions for the requested operation.', 'CloudFlow monitors CPU utilization (target: 70%) and memory usage (target: 80%) for its Horizontal Pod Autoscaling (HPA). Additionally, it collects key metrics including request rate, error rate, and latency (p50, p95, p99) for overall application performance monitoring.', 'To authenticate with CloudFlow APIs using an API key, follow these steps:\\n\\n1. Generate an API key in the CloudFlow dashboard or through the /api/v1/keys endpoint. The API key format should be \"cf_live_\" followed by 32 alphanumeric characters, making it a total of 40 characters.\\n\\n2. Include your API key in the Authorization header of your API requests. The format should be: \\n   ```\\n   Authorization: Bearer YOUR_API_KEY\\n   ```\\n\\n3. Ensure that all API requests are made over HTTPS, as HTTP requests will be rejected with a 403 error.\\n\\n4. Optionally, you can scope your API key to specific permissions (read, write, admin) and restrict it to specific IP addresses for enhanced security. \\n\\nMake sure to check the API key for any formatting issues, as it is case-sensitive.', 'To handle rate limit errors in CloudFlow, follow these steps:\\n\\n1. **Implement Exponential Backoff**: When you receive a \"Rate Limit Exceeded\" (HTTP 429) error, wait before retrying your request. Use an exponential backoff strategy: wait 1 second, then 2 seconds, then 4 seconds, etc.\\n\\n2. **Check Rate Limit Headers**: In the response headers, look for:\\n   - `X-RateLimit-Remaining`: Indicates how many requests you have left in the current window.\\n   - `X-RateLimit-Reset`: Provides the Unix timestamp when the limit resets.\\n\\n3. **Adjust Your Usage**: If you consistently hit rate limits, consider upgrading to a higher pricing tier or using batch endpoints to reduce the number of requests.\\n\\nBy following these practices, you can effectively manage and mitigate rate limit errors in your application.', \"To optimize CloudFlow API performance, follow these steps:\\n\\n1. **Implement Caching Strategy**:\\n   - Use CloudFlow's built-in Redis cache for frequently accessed data with a TTL between 5-60 minutes.\\n   - Cache API responses on the client side and respect Cache-Control headers.\\n\\n2. **Optimize Requests**:\\n   - Batch multiple operations into single API calls when possible.\\n   - Use pagination for large result sets (recommended page size: 50-100 items).\\n   - Implement request compression using gzip to reduce bandwidth.\\n\\n3. **Manage Connections**:\\n   - Reuse HTTP connections with keep-alive headers.\\n   - Maintain a connection pool with 5-10 concurrent connections per API key.\\n   - Set appropriate timeouts: connection timeout of 10 seconds and read timeout of 30 seconds.\\n\\n4. **Enhance Query Efficiency**:\\n   - Use field filtering to request only required data (e.g., `/resources?fields=id,name,status`).\\n   - Leverage server-side filtering instead of retrieving all data and filtering locally.\\n\\n5. **Utilize Asynchronous Processing**:\\n   - For long-running operations, use CloudFlow's async API endpoints.\\n   - Poll for results using the returned job_id rather than blocking on the initial request.\\n\\n6. **Serve Static Assets via CDN**:\\n   - Use CloudFlow's global CDN with 150+ edge locations for static assets.\\n   - Configure appropriate cache headers for optimal performance (e.g., max-age=3600 for semi-static content). \\n\\nBy following these best practices, you can achieve optimal performance from your CloudFlow applications.\", 'The Standard tier has a rate limit of 1,000 requests per hour per API key, with a burst capacity of up to 100 requests per minute. In contrast, the Premium tier allows for 10,000 requests per hour per API key, with a burst capacity of 500 requests per minute.', 'OAuth 2.0 authentication is recommended for user-facing applications and supports the Authorization Code flow, providing access tokens valid for 1 hour and refresh tokens valid for 30 days. Users must be directed to the authorization endpoint with the client_id and redirect_uri parameters.\\n\\nIn contrast, API key authentication is ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters and never expires unless explicitly revoked. API keys are included in the Authorization header as \"Authorization: Bearer YOUR_API_KEY\".\\n\\nAdditionally, all API requests must be made over HTTPS, and while OAuth 2.0 is more suited for user interactions, API keys are simpler and more direct for automated processes.', 'If you receive a 504 timeout error, increase the client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Additionally, check the CloudFlow status page for any service degradation.', 'To debug slow API response times in CloudFlow, follow these steps:\\n\\n1. **Check Service Status**: Visit status.cloudflow.io to ensure all systems are operational.\\n\\n2. **Review API Logs**: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Look for patterns in response times and any failed requests.\\n\\n3. **Enable Debug Mode**: Add the header `X-CloudFlow-Debug: true` to your requests. This will provide detailed debug information in the responses, including request ID and processing time breakdown.\\n\\n4. **Test with curl**: Use curl commands to isolate issues. For example:\\n   ```\\n   curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources\\n   ```\\n\\n5. **Check Network Connectivity**: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443.\\n\\n6. **Optimize Requests**: Implement request optimization strategies such as batching multiple operations into single API calls, using pagination for large result sets (recommended page size: 50-100 items), and enabling request compression with gzip.\\n\\n7. **Connection Management**: Reuse HTTP connections with keep-alive headers and maintain a connection pool with 5-10 concurrent connections per API key.\\n\\nBy following these steps, you can identify and address the causes of slow API response times.', \"I don't have enough information to answer that question.\", \"I don't have enough information to answer that question.\"], 'reference': ['CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones.', 'CloudFlow uses OAuth 2.0 for user-facing applications and API keys for server-to-server communication.', 'CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices.', 'CloudFlow supports SOC 2 Type II, GDPR, HIPAA, ISO 27001, and PCI DSS Level 1 compliance standards.', 'CloudFlow audit logs are immutable and retained for 2 years.', 'The three main layers are: API Gateway layer (handles authentication and routing), Service Mesh layer (orchestrates microservices), and Data Storage layer (distributed database with replication).', 'Standard tier costs $99/month with 1,000 requests/hour, Premium tier costs $499/month with 10,000 requests/hour, and Enterprise tier has custom pricing with 100,000+ requests/hour.', '401 Unauthorized indicates missing or invalid API key, and 403 Forbidden indicates valid API key but insufficient permissions.', 'CloudFlow monitors CPU utilization (target 70%), memory usage (target 80%), and triggers scaling when thresholds are exceeded for more than 3 consecutive minutes.', \"Include your API key in the Authorization header as 'Authorization: Bearer YOUR_API_KEY'. All requests must be made over HTTPS, and API keys have the format 'cf_live_' followed by 32 alphanumeric characters.\", 'When you receive a 429 error, implement exponential backoff in retry logic, check the X-RateLimit-Reset header to know when limits reset, and use the Retry-After header to determine wait time (1s, then 2s, then 4s, etc.).', 'Implement caching with Redis (TTL 5-60 minutes), batch multiple operations into single API calls, use pagination for large result sets (50-100 items), enable request compression with gzip, and maintain a connection pool with 5-10 concurrent connections.', 'Standard tier allows 1,000 requests per hour with 100 requests per minute burst, while Premium tier allows 10,000 requests per hour with 500 requests per minute burst. Premium also includes priority request processing.', 'OAuth 2.0 is recommended for user-facing applications with access tokens valid for 1 hour and provides the Authorization Code flow, while API keys are ideal for server-to-server communication, never expire unless revoked, and have a simpler implementation.', 'Increase client timeout to at least 30 seconds, use async endpoints for long-running operations and poll for results, and check the CloudFlow status page for any service degradation.', 'Add X-CloudFlow-Debug: true header to requests for detailed debug information, review API logs in the dashboard under Analytics > API Logs, test with curl commands, and verify network connectivity to *.cloudflow.io on port 443.', \"If you use an expired access token, you'll receive a 401 Unauthorized error. You should use your refresh token to obtain a new access token. Access tokens are valid for 1 hour and refresh tokens are valid for 30 days.\", \"I don't have enough information to answer that question.\"], 'retrieved_contexts': [['CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.', 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', 'CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.', \"CloudFlow Support Escalation Process\\n\\nUnderstanding CloudFlow's support escalation process ensures your issues are resolved efficiently.\\n\\nSupport Channels:\\n- Community Forums (All tiers): community.cloudflow.io - Best for general questions, feature requests, and sharing knowledge\\n- Email Support (Premium & Enterprise): support@cloudflow.io - Include account ID and request ID in subject line\\n- Phone Support (Enterprise only): +1-888-CLOUDFLOW - Available 24/7 for critical issues\\n- Slack Channel (Enterprise only): Direct access to engineering team\\n\\nIssue Severity Levels:\\n- P0 (Critical): Complete service outage affecting production. Response time: 1 hour for Enterprise, 4 hours for Premium\\n- P1 (High): Major functionality impaired but workarounds available. Response time: 4 hours for Enterprise, 8 hours for Premium\\n- P2 (Medium): Minor functionality issues with workarounds. Response time: 24 hours\\n- P3 (Low): Questions, feature requests, documentation issues. Response time: 48 hours\", \"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\"], ['CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', \"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\", \"Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\"], ['CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.', 'CloudFlow Scaling Mechanisms\\n\\nCloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\\n\\nHorizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\\n\\nVertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\\n\\nThe platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling.'], [\"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\", 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.'], ['Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\\n\\nVulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours.', 'Log Management: CloudFlow retains logs for 7 days by default (30 days for Premium, 90 days for Enterprise). Use structured logging with JSON format for better searchability. Maximum log line length: 32KB.\\n\\nDashboard Widgets: Create custom dashboards with real-time metrics, SLA compliance, and cost tracking. Share dashboards with team members or embed in external tools using iframe integration.', 'CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.', \"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\", 'Export Capabilities: Export your data anytime in JSON, CSV, or Parquet format. Full exports are available via the /api/v1/export endpoint. Large exports (>10 GB) are delivered to your S3 bucket.\\n\\nRTO and RPO: CloudFlow guarantees Recovery Time Objective (RTO) of 4 hours and Recovery Point Objective (RPO) of 1 hour for Enterprise tier. Contact support to initiate disaster recovery procedures.'], ['CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.'], ['CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', 'CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.', \"CloudFlow Billing Information\\n\\nUnderstanding CloudFlow's billing model helps you manage costs effectively and avoid unexpected charges.\\n\\nBilling Cycle: Subscriptions are billed monthly on the date you signed up. Annual subscriptions are billed upfront with a 15% discount. Billing date can be changed once per year.\\n\\nUsage-Based Charges: Beyond included quotas, additional usage is billed at:\\n- API requests: $0.01 per 1,000 requests\\n- Storage: $0.10 per GB per month\\n- Bandwidth: $0.08 per GB\\n- Backup retention (beyond 30 days): $0.05 per GB per month\\n\\nPayment Methods: CloudFlow accepts credit cards (Visa, Mastercard, Amex), ACH transfers (US only), and wire transfers for invoices over $1,000. Cryptocurrency payments available for annual plans.\\n\\nInvoicing: Invoices are emailed on the billing date and available in the dashboard. Enterprise customers receive consolidated monthly invoices with 30-day payment terms.\", 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.'], ['CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.', 'CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', \"Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\", 'Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}'], ['CloudFlow Scaling Mechanisms\\n\\nCloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\\n\\nHorizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\\n\\nVertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\\n\\nThe platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling.', 'CloudFlow Monitoring and Observability\\n\\nEffective monitoring ensures your CloudFlow applications remain healthy and performant.\\n\\nMetrics Collection: CloudFlow automatically collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization. Access metrics via the dashboard or Metrics API at /api/v1/metrics.\\n\\nCustom Metrics: Send custom application metrics using the StatsD protocol. CloudFlow aggregates custom metrics every 60 seconds and retains them for 90 days.\\n\\nAlerting: Configure alerts for critical conditions like error rate >5%, latency >500ms, or approaching rate limits. CloudFlow supports alerting via email, SMS, Slack, PagerDuty, and webhooks.\\n\\nDistributed Tracing: Enable distributed tracing to track requests across services. CloudFlow supports OpenTelemetry and provides trace visualization in the dashboard. Sample rate: 10% of requests (configurable up to 100%).', 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.'], ['CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', 'API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.'], ['CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.', 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}'], [\"CloudFlow Performance Optimization\\n\\nFollowing these best practices will help you achieve optimal performance from your CloudFlow applications.\\n\\nCaching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\\n\\nRequest Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\\n\\nConnection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\\n\\nQuery Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally.\", \"Asynchronous Processing: For long-running operations, use CloudFlow's async API endpoints. Poll for results using the returned job_id rather than blocking on the initial request.\\n\\nCDN Usage: Serve static assets through CloudFlow's global CDN with 150+ edge locations. Configure appropriate cache headers for optimal performance: max-age=3600 for semi-static content.\", 'CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.', 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options'], ['OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.', 'CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.', 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', 'Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\\n\\nFree Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.', 'All tiers include: SSL certificates, daily backups, API access, and dashboard analytics. Annual billing provides 15% discount.'], ['CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', 'API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.'], ['Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.', 'Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}', \"Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\", 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.'], ['CloudFlow Debugging Guide\\n\\nWhen troubleshooting issues with CloudFlow, follow this systematic debugging approach.\\n\\nStep 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\\n\\nStep 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\\n\\nStep 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\\n\\nStep 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources', 'Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.', \"CloudFlow Performance Optimization\\n\\nFollowing these best practices will help you achieve optimal performance from your CloudFlow applications.\\n\\nCaching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\\n\\nRequest Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\\n\\nConnection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\\n\\nQuery Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally.\", \"Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\", 'CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.'], ['OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.', 'CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\\n\\nFree Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.', 'Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.'], ['CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', \"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\"]]}\n"
     ]
    }
   ],
   "source": [
    "evaluation_dataset={\n",
    "    \"user_input\":[],\n",
    "    \"response\":[],\n",
    "    \"reference\":[],\n",
    "    \"retrieved_contexts\":[]\n",
    "}\n",
    "\n",
    "# for a csv File\n",
    "# Save to CSV\n",
    "#eval_dataset.to_csv(\"my_eval_dataset.csv\")\n",
    "# # After creating eval_dataset\n",
    "# with open(\"my_eval_dataset.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(eval_dataset, f)\n",
    "\n",
    "# print(\"âœ… Dataset saved as pickle\")\n",
    "# Load from CSV\n",
    "#loaded_dataset = EvaluationDataset.from_csv(\"my_eval_dataset.csv\")\n",
    "\n",
    "for references in test_cases:\n",
    "    user_input=references['question']\n",
    "    references=references['ground_truth']\n",
    "\n",
    "    response=chain.invoke(user_input)\n",
    "    retriever_docs=retriever.invoke(user_input)\n",
    "    retriever_docs=[doc.page_content for doc in retriever_docs]\n",
    "\n",
    "\n",
    "    evaluation_dataset['user_input'].append(user_input)\n",
    "    evaluation_dataset['response'].append(response)\n",
    "    evaluation_dataset['reference'].append(references)\n",
    "    evaluation_dataset['retrieved_contexts'].append(retriever_docs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9da31bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/7nk1nymx593bngn_fbl6n_fm0000gn/T/ipykernel_42334/2843326041.py:25: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use the modern LLM providers instead: from ragas.llms.base import llm_factory; llm = llm_factory('gpt-4o-mini') or from ragas.llms.base import instructor_llm_factory; llm = instructor_llm_factory('openai', client=openai_client)\n",
      "  ragas_llm=LangchainLLMWrapper(evaluator_llm)\n",
      "/var/folders/22/7nk1nymx593bngn_fbl6n_fm0000gn/T/ipykernel_42334/2843326041.py:26: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embedding=LangchainEmbeddingsWrapper(evaluator_embedding)\n"
     ]
    }
   ],
   "source": [
    "evaluation_docs=[\n",
    "    SingleTurnSample(\n",
    "        user_input=evaluation_dataset['user_input'][i],\n",
    "        response=evaluation_dataset['response'][i],\n",
    "        reference=evaluation_dataset['reference'][i],\n",
    "        retrieved_contexts=evaluation_dataset['retrieved_contexts'][i]\n",
    "    )\n",
    "    for i in range(0,len(evaluation_dataset['user_input']))\n",
    "]\n",
    "\n",
    "eval_set=EvaluationDataset(samples=evaluation_docs)\n",
    "\n",
    "evaluator_llm=ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    max_completion_tokens=3000,\n",
    "    timeout=480,\n",
    "    n=3\n",
    ")\n",
    "\n",
    "evaluator_embedding=OpenAIEmbeddings(\n",
    "    model='text-embedding-3-small'\n",
    ")\n",
    "\n",
    "ragas_llm=LangchainLLMWrapper(evaluator_llm)\n",
    "ragas_embedding=LangchainEmbeddingsWrapper(evaluator_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c7906421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/108 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   1%|          | 1/108 [00:12<22:50, 12.80s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   6%|â–Œ         | 6/108 [00:53<09:52,  5.81s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  15%|â–ˆâ–        | 16/108 [02:50<13:54,  9.07s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  16%|â–ˆâ–Œ        | 17/108 [03:06<16:41, 11.01s/it]Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating:  18%|â–ˆâ–Š        | 19/108 [03:08<09:34,  6.46s/it]Exception raised in Job[14]: TimeoutError()\n",
      "Exception raised in Job[12]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 24/108 [03:19<05:21,  3.83s/it]Exception raised in Job[16]: TimeoutError()\n",
      "Evaluating:  24%|â–ˆâ–ˆâ–       | 26/108 [03:33<06:49,  4.99s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 27/108 [03:49<09:39,  7.16s/it]Exception raised in Job[18]: TimeoutError()\n",
      "Evaluating:  29%|â–ˆâ–ˆâ–Š       | 31/108 [04:04<06:56,  5.41s/it]Exception raised in Job[22]: TimeoutError()\n",
      "Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 34/108 [04:48<11:35,  9.40s/it]Exception raised in Job[26]: TimeoutError()\n",
      "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 40/108 [05:35<10:27,  9.22s/it]Exception raised in Job[28]: TimeoutError()\n",
      "Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 42/108 [05:50<09:18,  8.46s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 43/108 [06:20<15:03, 13.90s/it]Exception raised in Job[32]: TimeoutError()\n",
      "Exception raised in Job[34]: TimeoutError()\n",
      "Exception raised in Job[40]: TimeoutError()\n",
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 47/108 [06:25<06:36,  6.50s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/108 [07:05<12:20, 12.34s/it]Exception raised in Job[44]: TimeoutError()\n",
      "Exception raised in Job[46]: TimeoutError()\n",
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 54/108 [07:13<04:49,  5.36s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 58/108 [07:55<06:59,  8.39s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 59/108 [08:04<06:59,  8.56s/it]Exception raised in Job[52]: TimeoutError()\n",
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 61/108 [08:13<05:28,  6.99s/it]Exception raised in Job[54]: TimeoutError()\n",
      "Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 64/108 [08:39<06:27,  8.80s/it]Exception raised in Job[56]: TimeoutError()\n",
      "Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/108 [08:49<04:58,  7.10s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 68/108 [09:33<09:39, 14.49s/it]Exception raised in Job[62]: TimeoutError()\n",
      "Exception raised in Job[58]: TimeoutError()\n",
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 72/108 [09:44<04:31,  7.53s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 74/108 [10:21<06:45, 11.93s/it]Exception raised in Job[68]: TimeoutError()\n",
      "Exception raised in Job[66]: TimeoutError()\n",
      "Exception raised in Job[64]: TimeoutError()\n",
      "Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 78/108 [10:28<03:15,  6.50s/it]Exception raised in Job[70]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 84/108 [11:10<03:20,  8.36s/it]Exception raised in Job[76]: TimeoutError()\n",
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 86/108 [11:14<02:17,  6.24s/it]Exception raised in Job[78]: TimeoutError()\n",
      "Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 87/108 [11:26<02:32,  7.26s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 88/108 [11:48<03:29, 10.49s/it]Exception raised in Job[80]: TimeoutError()\n",
      "Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 91/108 [11:52<01:44,  6.13s/it]Exception raised in Job[82]: TimeoutError()\n",
      "Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 93/108 [12:39<03:21, 13.44s/it]Exception raised in Job[86]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 97/108 [12:52<01:25,  7.80s/it]Exception raised in Job[88]: TimeoutError()\n",
      "Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 102/108 [13:30<00:54,  9.12s/it]Exception raised in Job[94]: TimeoutError()\n",
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 106/108 [13:44<00:11,  5.84s/it]Exception raised in Job[100]: TimeoutError()\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [14:26<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.7858, 'context_entity_recall': 0.3833, 'faithfulness': 0.8716, 'noise_sensitivity(mode=relevant)': 0.0000, 'answer_relevancy': 0.7831}\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import(\n",
    "    ContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    ContextPrecision,\n",
    "    Faithfulness,\n",
    "    NoiseSensitivity,\n",
    "    AnswerRelevancy\n",
    ")\n",
    "\n",
    "metrics=[\n",
    "    ContextPrecision(llm=ragas_llm),\n",
    "    ContextEntityRecall(llm=ragas_llm),\n",
    "    ContextPrecision(llm=ragas_llm),\n",
    "    Faithfulness(llm=ragas_llm),\n",
    "    NoiseSensitivity(llm=ragas_llm),\n",
    "    AnswerRelevancy(llm=ragas_llm,embeddings=evaluator_embedding)\n",
    "]\n",
    "\n",
    "result=evaluate(\n",
    "    dataset=eval_set,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479e519",
   "metadata": {},
   "source": [
    "LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84750e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Score(str,Enum):\n",
    "    \"\"\"0-3 for creating the Evaluation metrics\"\"\"\n",
    "    no_relevance=\"0\"\n",
    "    low_relevance=\"1\"\n",
    "    medium_relevance=\"2\"\n",
    "    high_relevance=\"3\"\n",
    "\n",
    "SCORE_DESCRIPTION = (\n",
    "    \"Score as a string between '0' and '3'. \"\n",
    "    \"0: No relevance/Not grounded/Poor quality - Completely fails the criterion. \"\n",
    "    \"1: Low relevance/Low groundedness/Below average - Minimal adherence to criterion. \"\n",
    "    \"2: Medium relevance/Medium groundedness/Good - Mostly meets the criterion. \"\n",
    "    \"3: High relevance/High groundedness/Excellent - Fully meets the criterion.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a60251c",
   "metadata": {},
   "source": [
    "Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a419e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Groundedness(BaseModel):\n",
    "     \"\"\"Evaluates if the answer is faithful to retrieved context (no hallucinations)\"\"\"\n",
    "     reasoning:str=Field(...,description=\"Check if the generated response is derived from the retrieved context ensuring the LLM doesnot Hallucinate.\")\n",
    "     score:Score=Field(...,description=SCORE_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3eefb",
   "metadata": {},
   "source": [
    "Answer Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41a4c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerRelevancy(BaseModel):\n",
    "    \"\"\"Checks if the answer given as response is relevant to the question asked as query\"\"\"\n",
    "    reasoning:str=Field(...,description=\"Check if the answer given as response is relevant to the question asked as query\")\n",
    "    score:Score=Field(...,description=SCORE_DESCRIPTION)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f113b",
   "metadata": {},
   "source": [
    "Retrieval Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c07d31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalQuality(BaseModel):\n",
    "    \"\"\"Evaluates if the retrieved contexts are helpful is responsing the query\"\"\"\n",
    "    reasoning:str=Field(...,description=\"Evaluate if the contexts retrieved from the sources are relevant and helpful in aswering the query asked by the user\")\n",
    "    score:Score=Field(...,description=SCORE_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85d2c3",
   "metadata": {},
   "source": [
    "RAG EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4dbdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG Evaluation Model\n",
    "class RAGEvaluation(BaseModel):\n",
    "    \"\"\"Complete RAG evaluation with all three metrics\"\"\"\n",
    "    groundedness: Groundedness = Field(\n",
    "        ...,\n",
    "        description=\"Evaluation of answer faithfulness to retrieved context\"\n",
    "    )\n",
    "    answer_relevance: AnswerRelevancy = Field(\n",
    "        ...,\n",
    "        description=\"Evaluation of answer relevance to the question\"\n",
    "    )\n",
    "    retrieval_quality: RetrievalQuality = Field(\n",
    "        ...,\n",
    "        description=\"Evaluation of retrieved context relevance to the question\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53822284",
   "metadata": {},
   "source": [
    "Judge Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3065b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for Retrieval-Augmented Generation (RAG) systems.\n",
      "Your role is to assess the quality of RAG system outputs across three key dimensions:\n",
      "\n",
      "1. **Groundedness**: How faithful is the answer to the retrieved context? Does it contain hallucinations?\n",
      "2. **Answer Relevance**: How well does the answer address the user's question?\n",
      "3. **Retrieval Quality**: How relevant are the retrieved contexts to answering the question?\n",
      "\n",
      "For each dimension:\n",
      "- Provide detailed step-by-step reasoning\n",
      "- Assign a score from 0-3 where:\n",
      "  - 0: Completely fails the criterion\n",
      "  - 1: Minimal adherence (significant issues)\n",
      "  - 2: Mostly meets criterion (minor issues)\n",
      "  - 3: Fully meets criterion (excellent quality)\n",
      "\n",
      "Be objective, thorough, and consistent in your evaluations.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt_template=\"\"\"You are an expert evaluator for Retrieval-Augmented Generation (RAG) systems.\n",
    "Your role is to assess the quality of RAG system outputs across three key dimensions:\n",
    "\n",
    "1. **Groundedness**: How faithful is the answer to the retrieved context? Does it contain hallucinations?\n",
    "2. **Answer Relevance**: How well does the answer address the user's question?\n",
    "3. **Retrieval Quality**: How relevant are the retrieved contexts to answering the question?\n",
    "\n",
    "For each dimension:\n",
    "- Provide detailed step-by-step reasoning\n",
    "- Assign a score from 0-3 where:\n",
    "  - 0: Completely fails the criterion\n",
    "  - 1: Minimal adherence (significant issues)\n",
    "  - 2: Mostly meets criterion (minor issues)\n",
    "  - 3: Fully meets criterion (excellent quality)\n",
    "\n",
    "Be objective, thorough, and consistent in your evaluations.\"\"\"\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1a9eb",
   "metadata": {},
   "source": [
    "Create the Format suitable for LLM judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4af46cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_judge_prompt(query:str,response:str,context:List[str])->str:\n",
    "    \"\"\"\n",
    "        Create the user prompt for the judge LLM.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        retrieved_contexts: List of retrieved document chunks\n",
    "        generated_answer: RAG system's answer\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "\n",
    "    context_format=\"\\n\\n--\\n\\n\".join(f\"Context {i+1}:\\n{context}\" for i,context in enumerate(context))\n",
    "\n",
    "    return \"\"\"Evaluate the following component of RAG\n",
    "    Question:{query}\n",
    "    Response:{response}\n",
    "    context:{context_format}\n",
    "\n",
    "    Please evaluate the Following component of RAG based on groundedness,answer_relevance,retrieval_quality\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc343a",
   "metadata": {},
   "source": [
    "Evaluate RAG with Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96a32b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_with_judge(query:str,context:List[str],response:str)->RAGEvaluation:\n",
    "        \"\"\"\n",
    "    Evaluate a single RAG interaction using LLM-as-Judge.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        retrieved_contexts: List of retrieved document chunks\n",
    "        generated_answer: RAG system's answer\n",
    "        \n",
    "    Returns:\n",
    "        RAGEvaluation object with scores and reasoning for all metrics\n",
    "    \"\"\"\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":prompt_template},\n",
    "            {\"role\":\"user\",\"content\":create_judge_prompt(query,context,response)}\n",
    "        ]\n",
    "\n",
    "        response=llm.with_structured_output(RAGEvaluation).invoke(messages)\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2d77b",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00167395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>) answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>) retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved contexts in answering the user's query.\", score=<Score.medium_relevance: '2'>)\n"
     ]
    }
   ],
   "source": [
    "test_question = \"What is CloudFlow's uptime SLA?\"\n",
    "test_answer =chain.invoke(test_question)\n",
    "test_contexts = [doc.page_content for doc in retriever.invoke(test_question)]\n",
    "\n",
    "\n",
    "response=evaluate_rag_with_judge(test_question,test_contexts,test_answer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5731b910",
   "metadata": {},
   "source": [
    "Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f0729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 18 test questions with ground truth answers\n",
      "\n",
      "Question breakdown by category:\n",
      "  - Simple Factual: 5 questions\n",
      "  - Multi-Fact: 4 questions\n",
      "  - Procedural: 3 questions\n",
      "  - Comparison: 2 questions\n",
      "  - Troubleshooting: 2 questions\n",
      "  - Edge Cases: 2 questions\n"
     ]
    }
   ],
   "source": [
    "# Test cases: question + ground truth answer pairs\n",
    "test_cases = [\n",
    "    # ========== SIMPLE FACTUAL (5) ==========\n",
    "    {\n",
    "        \"question\": \"What is CloudFlow's uptime SLA?\",\n",
    "        \"ground_truth\": \"CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What authentication protocol does CloudFlow use?\",\n",
    "        \"ground_truth\": \"CloudFlow uses OAuth 2.0 for user-facing applications and API keys for server-to-server communication.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the service mesh technology used by CloudFlow?\",\n",
    "        \"ground_truth\": \"CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What compliance standards does CloudFlow support?\",\n",
    "        \"ground_truth\": \"CloudFlow supports SOC 2 Type II, GDPR, HIPAA, ISO 27001, and PCI DSS Level 1 compliance standards.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long are CloudFlow audit logs retained?\",\n",
    "        \"ground_truth\": \"CloudFlow audit logs are immutable and retained for 2 years.\"\n",
    "    },\n",
    "    \n",
    "    # ========== MULTI-FACT (4) ==========\n",
    "    {\n",
    "        \"question\": \"What are the three main layers of CloudFlow architecture?\",\n",
    "        \"ground_truth\": \"The three main layers are: API Gateway layer (handles authentication and routing), Service Mesh layer (orchestrates microservices), and Data Storage layer (distributed database with replication).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are CloudFlow's pricing tiers and their API rate limits?\",\n",
    "        \"ground_truth\": \"Standard tier costs $99/month with 1,000 requests/hour, Premium tier costs $499/month with 10,000 requests/hour, and Enterprise tier has custom pricing with 100,000+ requests/hour.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What HTTP status codes indicate authentication failures in CloudFlow API?\",\n",
    "        \"ground_truth\": \"401 Unauthorized indicates missing or invalid API key, and 403 Forbidden indicates valid API key but insufficient permissions.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What auto-scaling metrics does CloudFlow monitor?\",\n",
    "        \"ground_truth\": \"CloudFlow monitors CPU utilization (target 70%), memory usage (target 80%), and triggers scaling when thresholds are exceeded for more than 3 consecutive minutes.\"\n",
    "    },\n",
    "    \n",
    "    # ========== PROCEDURAL (3) ==========\n",
    "    {\n",
    "        \"question\": \"How do I authenticate with CloudFlow APIs using an API key?\",\n",
    "        \"ground_truth\": \"Include your API key in the Authorization header as 'Authorization: Bearer YOUR_API_KEY'. All requests must be made over HTTPS, and API keys have the format 'cf_live_' followed by 32 alphanumeric characters.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I handle rate limit errors in CloudFlow?\",\n",
    "        \"ground_truth\": \"When you receive a 429 error, implement exponential backoff in retry logic, check the X-RateLimit-Reset header to know when limits reset, and use the Retry-After header to determine wait time (1s, then 2s, then 4s, etc.).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What steps should I follow to optimize CloudFlow API performance?\",\n",
    "        \"ground_truth\": \"Implement caching with Redis (TTL 5-60 minutes), batch multiple operations into single API calls, use pagination for large result sets (50-100 items), enable request compression with gzip, and maintain a connection pool with 5-10 concurrent connections.\"\n",
    "    },\n",
    "    \n",
    "    # ========== COMPARISON (2) ==========\n",
    "    {\n",
    "        \"question\": \"What's the difference between Standard and Premium tier rate limits?\",\n",
    "        \"ground_truth\": \"Standard tier allows 1,000 requests per hour with 100 requests per minute burst, while Premium tier allows 10,000 requests per hour with 500 requests per minute burst. Premium also includes priority request processing.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does OAuth 2.0 authentication differ from API key authentication in CloudFlow?\",\n",
    "        \"ground_truth\": \"OAuth 2.0 is recommended for user-facing applications with access tokens valid for 1 hour and provides the Authorization Code flow, while API keys are ideal for server-to-server communication, never expire unless revoked, and have a simpler implementation.\"\n",
    "    },\n",
    "    \n",
    "    # ========== TROUBLESHOOTING (2) ==========\n",
    "    {\n",
    "        \"question\": \"What should I do if I receive a 504 timeout error?\",\n",
    "        \"ground_truth\": \"Increase client timeout to at least 30 seconds, use async endpoints for long-running operations and poll for results, and check the CloudFlow status page for any service degradation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I debug slow API response times in CloudFlow?\",\n",
    "        \"ground_truth\": \"Add X-CloudFlow-Debug: true header to requests for detailed debug information, review API logs in the dashboard under Analytics > API Logs, test with curl commands, and verify network connectivity to *.cloudflow.io on port 443.\"\n",
    "    },\n",
    "    \n",
    "    # ========== EDGE CASES (2) ==========\n",
    "    {\n",
    "        \"question\": \"What happens if I use an expired OAuth token?\",\n",
    "        \"ground_truth\": \"If you use an expired access token, you'll receive a 401 Unauthorized error. You should use your refresh token to obtain a new access token. Access tokens are valid for 1 hour and refresh tokens are valid for 30 days.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does CloudFlow support blockchain integration?\",\n",
    "        \"ground_truth\": \"I don't have enough information to answer that question.\"  # Tests 'I don't know' handling\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(test_cases)} test questions with ground truth answers\\n\")\n",
    "print(\"Question breakdown by category:\")\n",
    "print(\"  - Simple Factual: 5 questions\")\n",
    "print(\"  - Multi-Fact: 4 questions\")\n",
    "print(\"  - Procedural: 3 questions\")\n",
    "print(\"  - Comparison: 2 questions\")\n",
    "print(\"  - Troubleshooting: 2 questions\")\n",
    "print(\"  - Edge Cases: 2 questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97017d1d",
   "metadata": {},
   "source": [
    "Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40a0942a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_input': \"What is CloudFlow's uptime SLA?\", 'response': \"CloudFlow's uptime SLA is guaranteed at 99.99% across availability zones.\", 'reference': 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones.', 'retriever_context': ['CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.', 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', 'CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.', \"CloudFlow Support Escalation Process\\n\\nUnderstanding CloudFlow's support escalation process ensures your issues are resolved efficiently.\\n\\nSupport Channels:\\n- Community Forums (All tiers): community.cloudflow.io - Best for general questions, feature requests, and sharing knowledge\\n- Email Support (Premium & Enterprise): support@cloudflow.io - Include account ID and request ID in subject line\\n- Phone Support (Enterprise only): +1-888-CLOUDFLOW - Available 24/7 for critical issues\\n- Slack Channel (Enterprise only): Direct access to engineering team\\n\\nIssue Severity Levels:\\n- P0 (Critical): Complete service outage affecting production. Response time: 1 hour for Enterprise, 4 hours for Premium\\n- P1 (High): Major functionality impaired but workarounds available. Response time: 4 hours for Enterprise, 8 hours for Premium\\n- P2 (Medium): Minor functionality issues with workarounds. Response time: 24 hours\\n- P3 (Low): Questions, feature requests, documentation issues. Response time: 48 hours\", \"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\"]}, {'user_input': 'What authentication protocol does CloudFlow use?', 'response': 'CloudFlow uses OAuth 2.0 and API Keys for authentication.', 'reference': 'CloudFlow uses OAuth 2.0 for user-facing applications and API keys for server-to-server communication.', 'retriever_context': ['CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', \"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\", \"Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\"]}, {'user_input': 'What is the service mesh technology used by CloudFlow?', 'response': 'CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices, providing service discovery, health checking, and automatic failover.', 'reference': 'CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices.', 'retriever_context': ['CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.', 'CloudFlow Scaling Mechanisms\\n\\nCloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\\n\\nHorizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\\n\\nVertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\\n\\nThe platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling.']}, {'user_input': 'What compliance standards does CloudFlow support?', 'response': 'CloudFlow supports SOC 2 Type II, GDPR Compliance, HIPAA, and ISO 27001 compliance standards.', 'reference': 'CloudFlow supports SOC 2 Type II, GDPR, HIPAA, ISO 27001, and PCI DSS Level 1 compliance standards.', 'retriever_context': [\"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\", 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.']}, {'user_input': 'How long are CloudFlow audit logs retained?', 'response': 'CloudFlow audit logs are retained for 2 years.', 'reference': 'CloudFlow audit logs are immutable and retained for 2 years.', 'retriever_context': ['Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\\n\\nVulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours.', 'Log Management: CloudFlow retains logs for 7 days by default (30 days for Premium, 90 days for Enterprise). Use structured logging with JSON format for better searchability. Maximum log line length: 32KB.\\n\\nDashboard Widgets: Create custom dashboards with real-time metrics, SLA compliance, and cost tracking. Share dashboards with team members or embed in external tools using iframe integration.', 'CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.', \"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\", 'Export Capabilities: Export your data anytime in JSON, CSV, or Parquet format. Full exports are available via the /api/v1/export endpoint. Large exports (>10 GB) are delivered to your S3 bucket.\\n\\nRTO and RPO: CloudFlow guarantees Recovery Time Objective (RTO) of 4 hours and Recovery Point Objective (RPO) of 1 hour for Enterprise tier. Contact support to initiate disaster recovery procedures.']}, {'user_input': 'What are the three main layers of CloudFlow architecture?', 'response': 'The three main layers of CloudFlow architecture are the API Gateway layer, the Service Mesh layer, and the Data Storage layer.', 'reference': 'The three main layers are: API Gateway layer (handles authentication and routing), Service Mesh layer (orchestrates microservices), and Data Storage layer (distributed database with replication).', 'retriever_context': ['CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.']}, {'user_input': \"What are CloudFlow's pricing tiers and their API rate limits?\", 'response': 'CloudFlow offers three pricing tiers: Standard Tier ($99/month), Premium Tier ($499/month), and Enterprise Tier (Custom pricing). The API rate limits for each tier are as follows:\\n- Standard Tier: 1,000 requests per hour per API key with burst capacity of up to 100 requests per minute.\\n- Premium Tier: 10,000 requests per hour per API key with burst capacity of up to 500 requests per minute.\\n- Enterprise Tier: Custom API rate limits negotiated based on usage patterns, typically starting at 100,000 requests per hour.', 'reference': 'Standard tier costs $99/month with 1,000 requests/hour, Premium tier costs $499/month with 10,000 requests/hour, and Enterprise tier has custom pricing with 100,000+ requests/hour.', 'retriever_context': ['CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', 'CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.', \"CloudFlow Billing Information\\n\\nUnderstanding CloudFlow's billing model helps you manage costs effectively and avoid unexpected charges.\\n\\nBilling Cycle: Subscriptions are billed monthly on the date you signed up. Annual subscriptions are billed upfront with a 15% discount. Billing date can be changed once per year.\\n\\nUsage-Based Charges: Beyond included quotas, additional usage is billed at:\\n- API requests: $0.01 per 1,000 requests\\n- Storage: $0.10 per GB per month\\n- Bandwidth: $0.08 per GB\\n- Backup retention (beyond 30 days): $0.05 per GB per month\\n\\nPayment Methods: CloudFlow accepts credit cards (Visa, Mastercard, Amex), ACH transfers (US only), and wire transfers for invoices over $1,000. Cryptocurrency payments available for annual plans.\\n\\nInvoicing: Invoices are emailed on the billing date and available in the dashboard. Enterprise customers receive consolidated monthly invoices with 30-day payment terms.\", 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.']}, {'user_input': 'What HTTP status codes indicate authentication failures in CloudFlow API?', 'response': '401 Unauthorized and 403 Forbidden.', 'reference': '401 Unauthorized indicates missing or invalid API key, and 403 Forbidden indicates valid API key but insufficient permissions.', 'retriever_context': ['CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.', 'CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', \"Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\", 'Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}']}, {'user_input': 'What auto-scaling metrics does CloudFlow monitor?', 'response': 'CloudFlow monitors CPU utilization (target: 70%) and memory usage (target: 80%) for auto-scaling decisions.', 'reference': 'CloudFlow monitors CPU utilization (target 70%), memory usage (target 80%), and triggers scaling when thresholds are exceeded for more than 3 consecutive minutes.', 'retriever_context': ['CloudFlow Scaling Mechanisms\\n\\nCloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\\n\\nHorizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\\n\\nVertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\\n\\nThe platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling.', 'CloudFlow Monitoring and Observability\\n\\nEffective monitoring ensures your CloudFlow applications remain healthy and performant.\\n\\nMetrics Collection: CloudFlow automatically collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization. Access metrics via the dashboard or Metrics API at /api/v1/metrics.\\n\\nCustom Metrics: Send custom application metrics using the StatsD protocol. CloudFlow aggregates custom metrics every 60 seconds and retains them for 90 days.\\n\\nAlerting: Configure alerts for critical conditions like error rate >5%, latency >500ms, or approaching rate limits. CloudFlow supports alerting via email, SMS, Slack, PagerDuty, and webhooks.\\n\\nDistributed Tracing: Enable distributed tracing to track requests across services. CloudFlow supports OpenTelemetry and provides trace visualization in the dashboard. Sample rate: 10% of requests (configurable up to 100%).', 'CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.']}, {'user_input': 'How do I authenticate with CloudFlow APIs using an API key?', 'response': 'To authenticate with CloudFlow APIs using an API key, you need to include your API key in the Authorization header of your request. The format should be \"Authorization: Bearer YOUR_API_KEY\". API keys are never expired unless explicitly revoked and can be scoped to specific permissions and restricted to specific IP addresses for enhanced security.', 'reference': \"Include your API key in the Authorization header as 'Authorization: Bearer YOUR_API_KEY'. All requests must be made over HTTPS, and API keys have the format 'cf_live_' followed by 32 alphanumeric characters.\", 'retriever_context': ['CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', 'API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.']}, {'user_input': 'How do I handle rate limit errors in CloudFlow?', 'response': 'To handle rate limit errors in CloudFlow, implement exponential backoff in your retry logic. Check the X-RateLimit-Reset header to know when the limits will reset. Consider upgrading to a higher tier if you consistently hit limits. Additionally, use batch endpoints to reduce the number of requests made.', 'reference': 'When you receive a 429 error, implement exponential backoff in retry logic, check the X-RateLimit-Reset header to know when limits reset, and use the Retry-After header to determine wait time (1s, then 2s, then 4s, etc.).', 'retriever_context': ['CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.', 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}']}, {'user_input': 'What steps should I follow to optimize CloudFlow API performance?', 'response': \"To optimize CloudFlow API performance, follow these steps:\\n1. Implement caching at multiple levels using CloudFlow's built-in Redis cache with a TTL between 5-60 minutes.\\n2. Batch multiple operations into single API calls and use pagination for large result sets (recommended page size: 50-100 items).\\n3. Reuse HTTP connections with keep-alive headers and maintain a connection pool with 5-10 concurrent connections per API key.\\n4. Use field filtering to request only required data and leverage server-side filtering.\\n5. For long-running operations, use CloudFlow's async API endpoints and poll for results using the returned job_id.\\n6. Serve static assets through CloudFlow's global CDN with appropriate cache headers for optimal performance.\\n7. Monitor rate limits based on your pricing tier and implement exponential backoff in retry logic when rate limited.\", 'reference': 'Implement caching with Redis (TTL 5-60 minutes), batch multiple operations into single API calls, use pagination for large result sets (50-100 items), enable request compression with gzip, and maintain a connection pool with 5-10 concurrent connections.', 'retriever_context': [\"CloudFlow Performance Optimization\\n\\nFollowing these best practices will help you achieve optimal performance from your CloudFlow applications.\\n\\nCaching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\\n\\nRequest Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\\n\\nConnection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\\n\\nQuery Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally.\", \"Asynchronous Processing: For long-running operations, use CloudFlow's async API endpoints. Poll for results using the returned job_id rather than blocking on the initial request.\\n\\nCDN Usage: Serve static assets through CloudFlow's global CDN with 150+ edge locations. Configure appropriate cache headers for optimal performance: max-age=3600 for semi-static content.\", 'CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.', 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options']}, {'user_input': \"What's the difference between Standard and Premium tier rate limits?\", 'response': 'The Standard tier has a rate limit of 1,000 requests per hour per API key with burst capacity of up to 100 requests per minute. The Premium tier, on the other hand, offers a higher rate limit of 10,000 requests per hour per API key with burst capacity of up to 500 requests per minute.', 'reference': 'Standard tier allows 1,000 requests per hour with 100 requests per minute burst, while Premium tier allows 10,000 requests per hour with 500 requests per minute burst. Premium also includes priority request processing.', 'retriever_context': ['OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.', 'CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.', 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', 'Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\\n\\nFree Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.', 'All tiers include: SSL certificates, daily backups, API access, and dashboard analytics. Annual billing provides 15% discount.']}, {'user_input': 'How does OAuth 2.0 authentication differ from API key authentication in CloudFlow?', 'response': 'OAuth 2.0 authentication in CloudFlow supports the Authorization Code flow, provides access tokens valid for 1 hour, and refresh tokens valid for 30 days. It is recommended for user-facing applications. On the other hand, API key authentication is ideal for server-to-server communication and background jobs. Each API key has a specific format and never expires unless explicitly revoked.', 'reference': 'OAuth 2.0 is recommended for user-facing applications with access tokens valid for 1 hour and provides the Authorization Code flow, while API keys are ideal for server-to-server communication, never expire unless revoked, and have a simpler implementation.', 'retriever_context': ['CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', 'API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.']}, {'user_input': 'What should I do if I receive a 504 timeout error?', 'response': 'If you receive a 504 timeout error, you should increase the client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Additionally, check the CloudFlow status page for any service degradation.', 'reference': 'Increase client timeout to at least 30 seconds, use async endpoints for long-running operations and poll for results, and check the CloudFlow status page for any service degradation.', 'retriever_context': ['Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.', 'Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}', \"Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\", 'Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn\\'t been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you\\'re using the correct API version (/v1).', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.']}, {'user_input': 'How do I debug slow API response times in CloudFlow?', 'response': 'To debug slow API response times in CloudFlow, follow these steps:\\n\\n1. Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests for detailed debug information.\\n2. Review API Logs: Access detailed API logs in the CloudFlow dashboard to analyze processing time breakdown.\\n3. Test with curl: Use curl commands to isolate issues and monitor response times.\\n4. Check Network Connectivity: Ensure outbound HTTPS traffic to *.cloudflow.io on port 443 is allowed.\\n5. Contact Support: If issues persist, contact CloudFlow support with the request ID for assistance.', 'reference': 'Add X-CloudFlow-Debug: true header to requests for detailed debug information, review API logs in the dashboard under Analytics > API Logs, test with curl commands, and verify network connectivity to *.cloudflow.io on port 443.', 'retriever_context': ['CloudFlow Debugging Guide\\n\\nWhen troubleshooting issues with CloudFlow, follow this systematic debugging approach.\\n\\nStep 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\\n\\nStep 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\\n\\nStep 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\\n\\nStep 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources', 'Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.', \"CloudFlow Performance Optimization\\n\\nFollowing these best practices will help you achieve optimal performance from your CloudFlow applications.\\n\\nCaching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\\n\\nRequest Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\\n\\nConnection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\\n\\nQuery Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally.\", \"Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\", 'CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.']}, {'user_input': 'What happens if I use an expired OAuth token?', 'response': 'If you use an expired OAuth token, the CloudFlow Platform will return a 401 Unauthorized error indicating that the token is invalid. You will need to obtain a new access token by following the OAuth 2.0 flow to authenticate your requests successfully.', 'reference': \"If you use an expired access token, you'll receive a 401 Unauthorized error. You should use your refresh token to obtain a new access token. Access tokens are valid for 1 hour and refresh tokens are valid for 30 days.\", 'retriever_context': ['OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.', 'CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.', 'CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what\\'s wrong.\\n- 404 Not Found: Requested resource doesn\\'t exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.', 'Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\\n\\nFree Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.', 'Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.']}, {'user_input': 'Does CloudFlow support blockchain integration?', 'response': \"I don't have enough information to answer that question.\", 'reference': \"I don't have enough information to answer that question.\", 'retriever_context': ['CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.', 'CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.', \"CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\", 'CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options', \"CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\"]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "evaluator_dataset=[]\n",
    "{\n",
    "    \"user_input\":[],\n",
    "    \"response\":[],\n",
    "    \"reference\":[],\n",
    "    \"retriever_context\":[]\n",
    "}\n",
    "\n",
    "for i,test in enumerate(test_cases):\n",
    "    user_input=test.get(\"question\")\n",
    "    response=chain.invoke(user_input)\n",
    "    reference=test.get(\"ground_truth\")\n",
    "    retriever_context=retriever.invoke(user_input)\n",
    "    retriever_docs=[doc.page_content for doc in retriever_context]\n",
    "\n",
    "    evaluator_dataset.append({\n",
    "    \"user_input\":user_input,\n",
    "    \"response\":response,\n",
    "    \"reference\":reference,\n",
    "    \"retriever_context\":retriever_docs\n",
    "})\n",
    "\n",
    "print(evaluator_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20fbba4",
   "metadata": {},
   "source": [
    "Judge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4f79aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The response should be faithful to the retrieved context without any hallucinations. The response should directly relate to the information present in the retrieved context.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer should directly address the user's question and provide relevant information based on the query.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieved context should be relevant and helpful in answering the user's question. It should provide necessary information for generating a meaningful response.\", score=<Score.high_relevance: '3'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved contexts in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved contexts in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved contexts in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved contexts in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved context in answering the user's query.\", score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The response should be faithful to the retrieved context without any hallucinations. I will compare the response with the provided context to assess the groundedness.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"I will evaluate how well the response addresses the user's question based on the provided query. The response should directly answer the question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning='I will assess the relevance and helpfulness of the retrieved context in answering the query. The context should provide relevant information to support the response.', score=<Score.medium_relevance: '2'>)), RAGEvaluation(groundedness=Groundedness(reasoning='The groundedness will be evaluated based on how faithfully the answer is derived from the retrieved context without any hallucinations.', score=<Score.medium_relevance: '2'>), answer_relevance=AnswerRelevancy(reasoning=\"The answer relevance will be assessed based on how well the response addresses the user's question.\", score=<Score.high_relevance: '3'>), retrieval_quality=RetrievalQuality(reasoning=\"The retrieval quality will be evaluated based on the relevance and helpfulness of the retrieved contexts in answering the user's query.\", score=<Score.medium_relevance: '2'>))]\n"
     ]
    }
   ],
   "source": [
    "judge_results=[]\n",
    "\n",
    "for items in evaluator_dataset:\n",
    "    evaluation=evaluate_rag_with_judge(\n",
    "        query=items[\"user_input\"],\n",
    "        context=items[\"retriever_context\"],\n",
    "        response=items[\"response\"]\n",
    "    )\n",
    "\n",
    "    judge_results.append(evaluation)\n",
    "\n",
    "print(judge_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
