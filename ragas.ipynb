{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89fff2a",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2db3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample,EvaluationDataset,evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,ResponseRelevancy,LLMContextPrecisionWithReference,LLMContextRecall,NoiseSensitivity,ContextEntityRecall\n",
    ")\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b21c9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "def run_async_function(cornjob):\n",
    "    \"\"\"Helper Function To Run Async Functions in Jupyter Notebooks\"\"\"\n",
    "    try:\n",
    "        loop=asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            nest_asyncio.apply()\n",
    "            return loop.run_until_complete(cornjob)\n",
    "        else:\n",
    "            return asyncio.run(cornjob)\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(cornjob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5ce64",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0109764",
   "metadata": {},
   "source": [
    "Faithfullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fed395b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context=[\n",
    "    \"LLMs are trained on large text datasets and do not have consciousness or personal beliefs.\"\n",
    "]\n",
    "\n",
    "test_response=\"No, LLMs do not have personal beliefs and only generate text based on learned patterns.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43a3f5",
   "metadata": {},
   "source": [
    "Generate Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109517aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LLMs do not have personal beliefs.\n",
      "2. LLMs only generate text based on learned patterns.\n"
     ]
    }
   ],
   "source": [
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "    \"\"\"Given the following response,extract all factual claims as a numbered list.\n",
    "    Each claim should be single, verifiable statement\n",
    "    \n",
    "    Response:{test_response}\n",
    "    Extract all Factual Claims\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "\n",
    "result=chain.invoke({\"test_response\":test_response})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a726c",
   "metadata": {},
   "source": [
    "Match Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'claim': 'LLMs do not have personal beliefs.', 'is_supported': True, 'Explanation': 'SUPPORTED\\n\\nExplanation: The context clearly states that LLMs do not have consciousness or personal beliefs, meaning they do not possess the ability to hold personal beliefs. Therefore, the claim that LLMs do not have personal beliefs is supported by the context.'}, {'claim': 'LLMs only generate text based on learned patterns.', 'is_supported': True, 'Explanation': 'SUPPORTED\\n\\nExplanation: The context mentions that LLMs do not have consciousness or personal beliefs, indicating that they generate text based solely on learned patterns rather than personal experiences or opinions. This supports the claim that LLMs only generate text based on learned patterns.'}, {'claim': 'Today is a Sunny day', 'is_supported': False, 'Explanation': 'NOT SUPPORTED\\n\\nExplanation: The context provided is about LLMs being trained on text datasets and not having consciousness or personal beliefs. It does not mention anything about the weather, so it does not support or relate to the claim that today is a sunny day.'}]\n"
     ]
    }
   ],
   "source": [
    "claims=[\n",
    "    \"LLMs do not have personal beliefs.\",\n",
    "    \"LLMs only generate text based on learned patterns.\",\n",
    "    \"Today is a Sunny day\"\n",
    "]\n",
    "prompt_template=ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the Claim and Context,verify if the claim is Supported by the context\n",
    "\n",
    "Claim:{Claim}\n",
    "Context:{Context}\n",
    "\n",
    "Answer with:\n",
    " - \"SUPPORTED\" if the context supports the claim\n",
    " - \"NOT SUPPORTED\" if the context doesn't support the claim\n",
    "\n",
    " Also give a Brief Explanation\n",
    " Verdict: \n",
    "\"\"\"\n",
    ")\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "\n",
    "verification_results=[]\n",
    "\n",
    "for claim in claims:\n",
    "    result=chain.invoke(\n",
    "        {\"Claim\":claim,\"Context\":test_context[0]}\n",
    "    )\n",
    "    is_supported=\"SUPPORTED\" in result.upper() and \"NOT SUPPORTED\" not in result.upper()\n",
    "    verification_results.append(\n",
    "        {\n",
    "            \"claim\":claim,\n",
    "            \"is_supported\":is_supported,\n",
    "            \"Explanation\":result\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(verification_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb61684",
   "metadata": {},
   "source": [
    "Calculate Faithfullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e38c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "supported_claims=sum(1 for r in verification_results if r[\"is_supported\"])\n",
    "total_claims=len(verification_results)\n",
    "\n",
    "faithfilness_score=supported_claims/total_claims\n",
    "print(f\"{faithfilness_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9e7de",
   "metadata": {},
   "source": [
    "Ragas Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e69afc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "failthfulness_sample=SingleTurnSample(\n",
    "    user_input=\"Do Large Language Models have personal beliefs?\",\n",
    "    response=test_response,\n",
    "    retrieved_contexts=test_context\n",
    ")\n",
    "\n",
    "faithfulness_metric=Faithfulness(llm=llm)\n",
    "faithfulness_score=run_async_function(faithfulness_metric.single_turn_ascore(failthfulness_sample))\n",
    "print(faithfilness_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c76defe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect Faithfulness (No hallucinations)\n",
      "0.50\n",
      "**************************************************\n",
      "Partial Faithfulness (Some hallucinations)\n",
      "0.00\n",
      "**************************************************\n",
      "Zero Faithfulness (Complete hallucination)\n",
      "0.00\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "faithfulness_examples = [\n",
    "    {\n",
    "        \"name\": \"Perfect Faithfulness (No hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was played on January 15, 1967 at the Los Angeles Memorial Coliseum.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial Faithfulness (Some hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was on January 15, 1967. The Green Bay Packers won 35-10 with Bart Starr as MVP.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Zero Faithfulness (Complete hallucination)\",\n",
    "        \"response\": \"The first Super Bowl was held in Miami in 1970 and attracted over 100,000 spectators.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "faithfulness_metric=Faithfulness(llm=llm)\n",
    "for items in faithfulness_examples:\n",
    "    faithfulness_sample=SingleTurnSample(\n",
    "        user_input=\"Tell me about the first Super Bowl\",\n",
    "        response=items[\"response\"],\n",
    "        retrieved_contexts=items[\"context\"]\n",
    "    )\n",
    "\n",
    "    faithfulness_score=run_async_function(faithfulness_metric.single_turn_ascore(faithfulness_sample))\n",
    "\n",
    "    print(items[\"name\"])\n",
    "    print(f\"{faithfulness_score:.2f}\")\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17ba6a",
   "metadata": {},
   "source": [
    "Answer Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72179ef",
   "metadata": {},
   "source": [
    "Hypothetical Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa10052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "question=\"Do Large Language Models have personal beliefs?\"\n",
    "llm_response=\"No, LLMs do not have personal beliefs and only generate text based on learned patterns.\"\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "From the Answer given generate Hypothecial questions which are Questions to the answer.Strictly ensure that to all the questions generated\n",
    "they should all have the same answer i.e the answer given\n",
    "\n",
    "Answer:{Answer}\n",
    "\n",
    "Construct 3 questions in a numbered manner like\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "\n",
    "result=chain.invoke({\"Answer\":llm_response})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5409eb",
   "metadata": {},
   "source": [
    "Cosine SImilarity Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fe412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "def calculate_cosine_similarity(vec1,vec2):\n",
    "    vec1=np.array(vec1)\n",
    "    vec2=np.array(vec2)\n",
    "    num=np.dot(vec1,vec2)\n",
    "    dem=norm(vec1)*norm(vec2)\n",
    "    return num/dem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8f28f",
   "metadata": {},
   "source": [
    "Calculate Similarities Of the Original Question to the Questions generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a35001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.5985796899687075), np.float64(0.4147325779462339), np.float64(0.46830935807152213)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "generated_questions=[\n",
    "    \"Do LLMs possess personal beliefs when generating text?\",\n",
    "    \"Are LLMs only capable of generating text based on learned patterns?\",\n",
    "    \"Can LLMs form their own personal opinions when generating text?\"\n",
    "]\n",
    "embeddings=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "original_embedding=embeddings.embed_query(question)\n",
    "\n",
    "similarity_index=[]\n",
    "for items in generated_questions:\n",
    "    sim=embeddings.embed_query(items)\n",
    "    cosine=calculate_cosine_similarity(original_embedding,sim)\n",
    "    similarity_index.append(cosine)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7b8f4",
   "metadata": {},
   "source": [
    "Mean of the Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "671c7317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49387387532882115\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.mean(similarity_index)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0787fa",
   "metadata": {},
   "source": [
    "Using the RAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a8831b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5850632477874373\n"
     ]
    }
   ],
   "source": [
    "response_relevancy_sample=SingleTurnSample(\n",
    "    user_input=question,\n",
    "    response=llm_response,\n",
    "    retrieved_contexts=test_context\n",
    ")\n",
    "\n",
    "response_relevancy_metric=ResponseRelevancy(\n",
    "    llm=llm,embeddings=embeddings\n",
    ")\n",
    "\n",
    "response_relevancy_score=run_async_function(response_relevancy_metric.single_turn_ascore(response_relevancy_sample))\n",
    "\n",
    "print(response_relevancy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b8d98",
   "metadata": {},
   "source": [
    "Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde68d8",
   "metadata": {},
   "source": [
    "Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9eeffb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is an LLM?\"\n",
    "response=\"An LLM is an AI model trained on large text datasets to generate and understand human language.\"\n",
    "\n",
    "chunks=[\n",
    "    \"The Eiffel Tower is located in Paris and was completed in 1889.\",\n",
    "    \"Python lists are mutable data structures used to store collections of items.\",\n",
    "    \"Large Language Models are trained on massive text data and can generate human-like responses.\",\n",
    "    \"LLMs are commonly used for tasks like question answering and text generation.\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e7daee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "For the Given Question and Answer verify if the retrieved chunk is relevant or not\n",
    "Question:{Question},\n",
    "Answer:{Answer}\n",
    "Chunk:{Chunk}\n",
    "If the chunk is relevant give RELEVANT else give IRRELEVANT\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "relevancy_matrix=[]\n",
    "for chunk in chunks:\n",
    "    result=chain.invoke(\n",
    "        {\n",
    "            \"Question\":query,\n",
    "            \"Answer\":response,\n",
    "            \"Chunk\":chunk\n",
    "        }\n",
    "    )\n",
    "    is_relevant=\"RELEVANT\" in result.upper() and \"IRRELEVANT\" not in result.upper()\n",
    "    relevancy_matrix.append(is_relevant)\n",
    "\n",
    "print(relevancy_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab44fb5",
   "metadata": {},
   "source": [
    "Precision count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50b18b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41666666666666663\n"
     ]
    }
   ],
   "source": [
    "\n",
    "precision_good=[]\n",
    "relevant_count=0\n",
    "for k,relevancy in enumerate(relevancy_matrix,1):\n",
    "    if relevancy:\n",
    "        relevant_count+=1\n",
    "        precision_at_k=relevant_count/k\n",
    "        precision_good.append(precision_at_k)\n",
    "\n",
    "\n",
    "\n",
    "total_relevancy_matrix=sum(relevancy_matrix)\n",
    "\n",
    "total_context_precision=sum(precision_good)/sum(relevancy_matrix) if total_relevancy_matrix>0 else 0\n",
    "print(total_context_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5eb572",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9443e1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999999995\n",
      "0.4166666666458333\n"
     ]
    }
   ],
   "source": [
    "bad_sample=[\n",
    "    \"The Eiffel Tower is located in Paris and was completed in 1889.\",\n",
    "    \"Python lists are mutable data structures used to store collections of items.\",\n",
    "    \"Large Language Models are trained on massive text data and can generate human-like responses.\",\n",
    "    \"LLMs are commonly used for tasks like question answering and text generation.\"\n",
    "]\n",
    "\n",
    "good_sample=[\n",
    "    \"Large Language Models are trained on massive text data and can generate human-like responses.\",\n",
    "    \"LLMs are commonly used for tasks like question answering and text generation.\",\n",
    "    \"The Eiffel Tower is located in Paris and was completed in 1889.\",\n",
    "    \"Python lists are mutable data structures used to store collections of items.\"\n",
    "]\n",
    "\n",
    "context_precision_sample_good=SingleTurnSample(\n",
    "    user_input=query,\n",
    "    reference=response,\n",
    "    retrieved_contexts=good_sample\n",
    ")\n",
    "context_precision_sample_bad=SingleTurnSample(\n",
    "    user_input=query,\n",
    "    reference=response,\n",
    "    retrieved_contexts=bad_sample\n",
    ")\n",
    "\n",
    "context_precision_metric=LLMContextPrecisionWithReference(llm=llm)\n",
    "good_result=run_async_function(context_precision_metric.single_turn_ascore(context_precision_sample_good))\n",
    "bad_result=run_async_function(context_precision_metric.single_turn_ascore(context_precision_sample_bad))\n",
    "print(good_result)\n",
    "print(bad_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537c895",
   "metadata": {},
   "source": [
    "Conext Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1904bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim is The Eifel Tower is Located in Paris and is True\n",
      "Claim is It was buit in 1889 and is True\n",
      "Claim is It is 330 meters tall and is False\n"
     ]
    }
   ],
   "source": [
    "query=\"Tell me about Eiffel Tower\"\n",
    "recall_reference=\"The Eifel Tower is Located in Paris.It was buit in 1889.It is 330 meters tall\"\n",
    "\n",
    "recall_context=[\n",
    "    \"The Eifel Tower is a Landmark Located in Paris,France\",\n",
    "    \"The Tower was completed in 1889or the World's Fair\"\n",
    "]\n",
    "recall_claims=[\n",
    "    \"The Eifel Tower is Located in Paris\",\n",
    "    \"It was buit in 1889\",\n",
    "    \"It is 330 meters tall\"\n",
    "]\n",
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Check if the the Following Claims can be attributed to the Context\n",
    "    Claim:{Claim}\n",
    "    Context:{Context}\n",
    "\n",
    "    if the Claim is supported by the Context gives \"YES\" if not then gine \"NO\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "combined_context=\"\\n\".join(recall_context)\n",
    "claims=[]\n",
    "for claim in recall_claims:\n",
    "    result=chain.invoke({\n",
    "        \"Claim\":claim,\n",
    "        \"Context\":combined_context\n",
    "    })\n",
    "    is_supported=\"YES\" in result.upper() and \"NO\" not in result.upper()\n",
    "    claims.append(is_supported)\n",
    "    print(f\"Claim is {claim} and is {is_supported}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672afc16",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b9e0d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "context_recall_sample=SingleTurnSample(\n",
    "    user_input=query,\n",
    "    reference=recall_reference,\n",
    "    retrieved_contexts=recall_context\n",
    ")\n",
    "\n",
    "context_recall_metric=LLMContextRecall(llm=llm)\n",
    "context_recall_score=run_async_function(context_recall_metric.single_turn_ascore(context_recall_sample))\n",
    "\n",
    "print(context_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096c806",
   "metadata": {},
   "source": [
    "Context Entity Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f6c7fccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: William Shakespeare\n",
      "LOCATION: England\n",
      "LOCATION: 16th\n",
      "LOCATION: 17th century\n",
      "PLAY: Hamlet\n",
      "PLAY: Romeo and Juliet\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "PERSON: William Shakespeare\n",
      "LOCATION: English\n",
      "DATE: N/A\n",
      "WORK_OF_ART: literature\n"
     ]
    }
   ],
   "source": [
    "entity_reference = \"William Shakespeare wrote many famous plays such as Hamlet and Romeo and Juliet in England during the late 16th and early 17th century.\"\n",
    "entity_context = [\n",
    "    \"William Shakespeare was an English playwright known for his influential works in literature.\"\n",
    "]\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Extract all named entities from the following Text.\n",
    "Include: PERSON,NAME,DATE,LOCATION and other proper nouns\n",
    "Text:{Text}\n",
    "List each entity on a new line with its type:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template|llm|StrOutputParser()\n",
    "reference_result=chain.invoke({\"Text\":entity_reference})\n",
    "context_result=chain.invoke({\"Text\":entity_context[0]})\n",
    "print(reference_result)\n",
    "print('@'*50)\n",
    "print(context_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d7164b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "reference_entities = {\n",
    "    \"William Shakespeare\": \"PERSON\",\n",
    "    \"Hamlet\": \"PERSON\",\n",
    "    \"Romeo\": \"PERSON\",\n",
    "    \"Juliet\": \"PERSON\",\n",
    "    \"England\": \"LOCATION\",\n",
    "    \"late 16th and early 17th century\": \"DATE\"\n",
    "}\n",
    "\n",
    "context_entities = {\n",
    "    \"William Shakespeare\": \"PERSON\",\n",
    "    \"English\": \"LOCATION\",\n",
    "    \"playwright\": \"PERSON\"\n",
    "}\n",
    "\n",
    "\n",
    "common_entities_reference_context=reference_entities.keys() and context_entities.keys()\n",
    "common_entities_reference_context_size=len(common_entities_reference_context)\n",
    "reference_entities_size=len(reference_entities.keys())\n",
    "context_entity_recall=common_entities_reference_context_size/reference_entities_size\n",
    "\n",
    "print(context_entity_recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6725a",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d6ac6f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999996\n"
     ]
    }
   ],
   "source": [
    "context_entity_recall_sample=SingleTurnSample(\n",
    "    reference=entity_reference,\n",
    "    retrieved_contexts=entity_context\n",
    ")\n",
    "\n",
    "context_entity_recall_metric=ContextEntityRecall(llm=llm)\n",
    "context_entity_recall=run_async_function(context_entity_recall_metric.single_turn_ascore(context_entity_recall_sample))\n",
    "print(context_entity_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9675b",
   "metadata": {},
   "source": [
    "Noise Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4075a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_question = \"What is LIC known for?\"\n",
    "noise_response = \"LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.\"\n",
    "noise_reference = \"LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.\"\n",
    "\n",
    "noise_contexts = [\n",
    "    \"LIC was established in 1956 following nationalization.\",      \n",
    "    \"LIC is the largest insurance company with huge investments.\",     \n",
    "    \"LIC manages substantial funds for financial stability.\",           \n",
    "    \"The Indian economy is one of the fastest-growing economies...\"     \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b450383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContextLIC was established in 1956 following nationalization. True\n",
      "ContextLIC is the largest insurance company with huge investments. TRUE\n",
      "ContextLIC manages substantial funds for financial stability. TRUE\n",
      "ContextThe Indian economy is one of the fastest-growing economies... FALSE\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "For the reference given Below verify if the a given context directly support the reference or not\n",
    "\n",
    "reference:{reference}\n",
    "context:{context}\n",
    "\n",
    "return TRUE if the context directly supports the reference or FALSE if it doesnot directly supports the reference\n",
    "\"\"\"\n",
    ")\n",
    "chain=prompt_template|llm|StrOutputParser()\n",
    "for context in noise_contexts:\n",
    "    result=chain.invoke(\n",
    "        {\n",
    "            \"reference\":noise_reference,\n",
    "            \"context\":context\n",
    "        }\n",
    "    )\n",
    "    print(f\"Context{context} {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c4fec3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "response_claims = [\n",
    "    (\"LIC is the largest insurance company in India\", True, \"Matches reference\"),\n",
    "    (\"LIC is known for its vast portfolio\", True, \"Matches reference (portfolio)\"),\n",
    "    (\"LIC contributes to financial stability\", False, \"NOT in reference - possible hallucination from noise!\")\n",
    "]\n",
    "\n",
    "incorrect_count = 0\n",
    "for claim in response_claims:\n",
    "    if not claim[1]:\n",
    "        incorrect_count+=1\n",
    "\n",
    "noise=incorrect_count/len(response_claims)\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa7f31",
   "metadata": {},
   "source": [
    "RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f710da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "noise_sample=SingleTurnSample(\n",
    "    user_input=\"What is LIC known for?\",\n",
    "    response=\"LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.\",\n",
    "    reference=\"LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.\",\n",
    "    retrieved_contexts=noise_contexts\n",
    ")\n",
    "\n",
    "noise_metric=NoiseSensitivity(llm=llm)\n",
    "noise=run_async_function(noise_metric.single_turn_ascore(noise_sample))\n",
    "\n",
    "print(noise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
